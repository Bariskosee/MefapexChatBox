"""
Enhanced Content Manager for MEFAPEX Chatbot
Handles static responses from JSON files + Hugging Face AI integration + Hybrid Relevance Detection
"""

import json
import os
import logging
import asyncio
from typing import Dict, List, Optional, Tuple

# Import hybrid relevance detector
try:
    from hybrid_relevance_detector import HybridRelevanceDetector, RelevanceLevel
    RELEVANCE_DETECTOR_AVAILABLE = True
except ImportError as e:
    RELEVANCE_DETECTOR_AVAILABLE = False
    HybridRelevanceDetector = None
    RelevanceLevel = None

logger = logging.getLogger(__name__)

class ContentManager:
    """
    Enhanced content management system:
    - Static responses from JSON files (primary)
    - Hugging Face AI integration (secondary)
    - Hybrid relevance detection for off-topic questions
    - Intelligent keyword matching
    - Response caching for performance
    """
    
    def __init__(self, content_dir: str = "content"):
        self.content_dir = content_dir
        self.static_responses = {}
        self.categories = {}
        self.settings = {}
        self._cache = {}
        self._cache_enabled = True
        self._ai_enabled = True
        self._relevance_detection_enabled = True
        
        # Initialize hybrid relevance detector
        if RELEVANCE_DETECTOR_AVAILABLE:
            try:
                self.relevance_detector = HybridRelevanceDetector("MEFAPEX Bili≈üim Teknolojileri")
                logger.info("‚úÖ Hybrid relevance detection enabled")
            except Exception as e:
                self.relevance_detector = None
                self._relevance_detection_enabled = False
                logger.warning(f"‚ö†Ô∏è Relevance detector initialization failed: {e}")
        else:
            self.relevance_detector = None
            self._relevance_detection_enabled = False
            logger.warning("‚ö†Ô∏è Hybrid relevance detector not available")
        
        # Import model manager for AI responses
        try:
            from model_manager import model_manager
            self.model_manager = model_manager
            logger.info("‚úÖ Hugging Face AI integration enabled")
        except ImportError as e:
            self.model_manager = None
            self._ai_enabled = False
            logger.warning(f"‚ö†Ô∏è Hugging Face AI not available: {e}")
        
        # Load static content on initialization
        self.load_static_content()
        
    def load_static_content(self) -> bool:
        """Load static responses from JSON file"""
        try:
            json_path = os.path.join(self.content_dir, "static_responses.json")
            
            if not os.path.exists(json_path):
                logger.warning(f"Static responses file not found: {json_path}")
                return False
                
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            self.static_responses = data.get("responses", {})
            self.categories = data.get("categories", {})
            self.settings = data.get("settings", {})
            
            logger.info(f"‚úÖ Loaded {len(self.static_responses)} static responses")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load static content: {e}")
            return False

    def find_response(self, user_message: str) -> Tuple[str, str]:
        """
        Find appropriate response for user message with relevance detection
        Flow: Relevance Check -> Static -> Hugging Face AI -> Default
        Returns: (response_text, source)
        Source can be: irrelevant, static, cache_static, huggingface, cache_huggingface, default
        """
        if not user_message or not user_message.strip():
            return self._get_default_response(user_message), "default"
        
        user_message_lower = user_message.lower().strip()
        
        # Check cache first (for performance)
        if self._cache_enabled and user_message_lower in self._cache:
            cached_response, source = self._cache[user_message_lower]
            logger.debug(f"üéØ Cache hit for: {user_message[:30]}...")
            return cached_response, f"cache_{source}"
        
        # Check relevance using hybrid detector (if enabled)
        if self._relevance_detection_enabled and self.relevance_detector:
            try:
                # Check if we're already in an event loop
                try:
                    loop = asyncio.get_running_loop()
                    # We're in an event loop, create a task
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(self._run_relevance_check, user_message)
                        relevance_result = future.result(timeout=2.0)  # 2 second timeout
                except RuntimeError:
                    # No event loop running, we can use asyncio.run
                    relevance_result = asyncio.run(self._check_relevance_async(user_message))
                
                # If question is clearly irrelevant, return smart response immediately
                if (relevance_result and not relevance_result.is_relevant and 
                    relevance_result.confidence > 0.7 and
                    relevance_result.response_suggestion):
                    
                    logger.info(f"üö´ Irrelevant question detected ({relevance_result.confidence:.3f}): {user_message[:50]}...")
                    logger.info(f"üìä Method: {relevance_result.method.value}, Level: {relevance_result.relevance_level.value}")
                    
                    # Cache the irrelevant response
                    if self._cache_enabled:
                        self._cache[user_message_lower] = (relevance_result.response_suggestion, "irrelevant")
                    
                    return relevance_result.response_suggestion, "irrelevant"
                
                # Log relevance analysis for debugging
                if relevance_result:
                    logger.debug(f"üéØ Relevance: {relevance_result.is_relevant} "
                               f"({relevance_result.confidence:.3f}) - {relevance_result.method.value}")
                
            except Exception as e:
                logger.error(f"‚ùå Relevance detection failed: {e}")
                # Continue with normal processing
        
        # Try static responses first (highest priority)
        response, source = self._find_static_response(user_message_lower)
        
        if response:
            # Cache the response
            if self._cache_enabled:
                self._cache[user_message_lower] = (response, source)
            return response, source
        
        # Try Hugging Face AI if static response not found
        # AI with strict quality control enabled
        ai_enabled_for_testing = True  # Set to False to disable AI
        
        if self._ai_enabled and self.model_manager and ai_enabled_for_testing:
            try:
                ai_response = self._generate_ai_response(user_message)
                if ai_response and ai_response.strip() and len(ai_response.strip()) > 15:
                    logger.info(f"ü§ñ AI response generated for: {user_message[:30]}...")
                    # Cache the AI response
                    if self._cache_enabled:
                        self._cache[user_message_lower] = (ai_response, "huggingface")
                    return ai_response, "huggingface"
                else:
                    logger.debug(f"ü§ñ AI response quality insufficient, using fallback for: {user_message[:30]}...")
            except Exception as e:
                logger.error(f"‚ùå AI response generation failed: {e}")
        
        # Return intelligent default response as fallback
        intelligent_response = self._get_intelligent_default_response(user_message)
        if self._cache_enabled:
            self._cache[user_message_lower] = (intelligent_response, "intelligent_default")
        
        return intelligent_response, "intelligent_default"
    
    async def _check_relevance_async(self, user_message: str):
        """
        Async wrapper for relevance detection
        """
        if self.relevance_detector:
            return await self.relevance_detector.classify(user_message)
        return None
    
    def _run_relevance_check(self, user_message: str):
        """
        Synchronous wrapper to run relevance check in thread
        """
        return asyncio.run(self._check_relevance_async(user_message))

    def _find_static_response(self, user_message_lower: str) -> Tuple[Optional[str], str]:
        """Find matching static response"""
        best_match = None
        best_score = 0
        
        for category, response_data in self.static_responses.items():
            if isinstance(response_data, dict):
                keywords = response_data.get("keywords", [])
                response_text = response_data.get("message", "")
                
                # Calculate match score
                score = self._calculate_match_score(user_message_lower, keywords)
                
                if score > best_score:
                    best_match = response_text
                    best_score = score
                    
                # Also check for category name match
                if category.lower() in user_message_lower:
                    return response_text, "static"
            elif isinstance(response_data, str):
                # Simple string response
                if category.lower() in user_message_lower:
                    return response_data, "static"
        
        if best_match and best_score > 0.2:  # Minimum threshold
            return best_match, "static"
        
        return None, ""

    def _generate_ai_response(self, user_message: str) -> Optional[str]:
        """
        Generate AI response using Hugging Face models with improved quality control
        """
        try:
            if not self.model_manager:
                return None
            
            # Limit message length for processing
            message = user_message.strip()[:200]
            
            # Create a better prompt for Turkish context
            enhanced_prompt = self._create_enhanced_prompt(message)
            
            # Generate response using Turkish-optimized models
            ai_response = self.model_manager.generate_text_response(
                prompt=enhanced_prompt,
                max_length=120,  # Slightly longer for better context
                turkish_context=True
            )
            
            if ai_response and len(ai_response.strip()) > 10:
                # Clean and validate the response
                cleaned_response = self._clean_and_validate_ai_response(ai_response, user_message)
                if cleaned_response:
                    return cleaned_response
            
            # If AI response is poor quality, return None to trigger fallback
            return None
            
        except Exception as e:
            logger.error(f"AI response generation error: {e}")
            return None
    
    def _create_enhanced_prompt(self, user_message: str) -> str:
        """
        Create an enhanced prompt for better AI responses
        """
        # Detect question type and create appropriate context
        if any(word in user_message.lower() for word in ['hava', 'weather', 'meteoroloji']):
            return f"MEFAPEX m√º≈üteri destek asistanƒ± olarak hava durumu sorusuna kƒ±sa ve profesyonel yanƒ±t ver: {user_message}"
        
        elif any(word in user_message.lower() for word in ['nasƒ±l', 'nedir', 'ne', 'kim', 'nerede', 'ne zaman']):
            return f"MEFAPEX teknoloji firmasƒ± destek uzmanƒ± olarak bu soruya yardƒ±mcƒ± ve bilgilendirici yanƒ±t ver: {user_message}"
        
        elif any(word in user_message.lower() for word in ['√∂ƒüren', 'program', 'kod', 'yazƒ±lƒ±m', 'teknoloji']):
            return f"MEFAPEX teknoloji uzmanƒ± olarak yazƒ±lƒ±m/teknoloji konusundaki bu soruya rehberlik et: {user_message}"
        
        else:
            return f"MEFAPEX m√º≈üteri hizmetleri asistanƒ± olarak bu konuda yardƒ±mcƒ± ol: {user_message}"
    
    def _clean_and_validate_ai_response(self, ai_response: str, original_message: str) -> Optional[str]:
        """
        Clean, validate and improve AI response quality
        """
        try:
            # Remove the original prompt if it appears in response
            response = ai_response.strip()
            
            # Remove common prompt artifacts
            prompt_artifacts = [
                "MEFAPEX m√º≈üteri destek asistanƒ± olarak",
                "MEFAPEX teknoloji firmasƒ± destek uzmanƒ± olarak", 
                "MEFAPEX teknoloji uzmanƒ± olarak",
                "MEFAPEX m√º≈üteri hizmetleri asistanƒ± olarak",
                "olarak bu soruya", "olarak hava durumu", "olarak yazƒ±lƒ±m",
                "yanƒ±t ver:", "yardƒ±mcƒ± ol:", "rehberlik et:"
            ]
            
            for artifact in prompt_artifacts:
                if artifact in response:
                    response = response.replace(artifact, "").strip()
            
            # Remove the original message if it appears at the start
            if original_message.lower() in response.lower()[:50]:
                idx = response.lower().find(original_message.lower())
                if idx >= 0:
                    response = response[idx + len(original_message):].strip()
            
            # Clean up common prefixes
            prefixes_to_remove = [
                "t√ºrk√ße:", "turkish:", "cevap:", "response:", "yanƒ±t:", "answer:",
                "soru:", "question:", "merhaba", "selam", ":"
            ]
            
            for prefix in prefixes_to_remove:
                if response.lower().startswith(prefix):
                    response = response[len(prefix):].strip()
            
            # Quality checks
            if len(response) < 15:
                logger.debug("AI response too short, rejecting")
                return None
            
            # Check for repetitive patterns (sign of poor generation)
            words = response.split()
            if len(words) > 3:
                if len(set(words[:5])) <= 2:  # Too repetitive
                    logger.debug("AI response too repetitive, rejecting")
                    return None
            
            # Check for training data artifacts more strictly
            if any(artifact in response.lower() for artifact in [
                'fiyat', 'g√ºncellendi', 'web site', 'aracƒ±lƒ±ƒüƒ±yla', 'sunulmaktadƒ±r',
                'cumhuriyet gazetesi', 'yazarƒ±', 'k√∂≈üe yazƒ±sƒ±', 'hutbe',
                'domuz eti', 'imam', 'vaaz', 'cuma namazƒ±', 'anasayfa',
                'm√º≈üteri temsilcisi', '0850', 'numaralƒ±', '≈üikayetiniz',
                'en √ßok sorulan', 'sƒ±caklƒ±ƒüƒ± ka√ß', 'derece', 'telefon',
                'ula≈üabilirsiniz', 'aklƒ±nƒ±za takƒ±lan'
            ]):
                logger.debug("AI response contains training artifacts, rejecting")
                return None
            
            # Check for nonsensical patterns
            nonsense_patterns = [
                "###", "***", "===", "---", "///",
                "√ßok √ßok √ßok", "ve ve ve", "bir bir bir",
                "gibi gibi", "i√ßin i√ßin", "ile ile ile"
            ]
            
            if any(pattern in response.lower() for pattern in nonsense_patterns):
                logger.debug("AI response contains nonsense patterns, rejecting")
                return None
            
            # Check if response seems relevant to the question
            if not self._is_response_relevant(original_message, response):
                logger.debug("AI response not relevant to question, rejecting")
                return None
            
            # Additional quality checks for Turkish responses
            if not self._passes_quality_checks(response):
                logger.debug("AI response failed quality checks, rejecting")
                return None
            
            # Format the response with MEFAPEX branding if appropriate
            if len(response) > 20 and not any(brand in response.lower() for brand in ['mefapex', 'destek', 'yardƒ±m']):
                formatted_response = f"ü§ñ **MEFAPEX AI Asistanƒ±:**\n\n{response}\n\nüí° Daha detaylƒ± bilgi i√ßin destek ekibimizle ileti≈üime ge√ßebilirsiniz."
                return formatted_response
            
            return response
            
        except Exception as e:
            logger.error(f"Response cleaning error: {e}")
            return None
    
    def _passes_quality_checks(self, response: str) -> bool:
        """
        Additional quality checks for AI responses
        """
        try:
            # Check for common signs of poor quality
            lower_response = response.lower()
            
            # Check for incomplete sentences (no proper ending)
            if not response.strip().endswith(('.', '!', '?', ':', 'üöÄ', 'üí°', 'üåü', '‚ú®')):
                return False
            
            # Check for training artifacts more thoroughly
            training_artifacts = [
                'mefapex m√º≈üteri destek asistanƒ±',
                'mefapex m√º≈üteri hizmet',
                'fiyatlar g√ºncellendi',
                'm√º≈üteri hizmetlerine baƒülan',
                'mefepro',
                'sana yardƒ±m etmek i√ßin',
                'bir sonraki',
                '≈üimdi bu',
                'diyebilirsin',
                'gerekli malzeme',
                'tamam, ≈üimdi',
                'hazƒ±r olduƒüunu bilmen',
                'yemek tarifi ver',
                'bahsettiklerinde',
                'aradƒ±ƒüƒ±nda',
                'pi≈üirmek i√ßin',
                'alacaƒüƒ±m'
            ]
            
            for artifact in training_artifacts:
                if artifact in lower_response:
                    return False
            
            # Check for broken sentences that start oddly
            bad_starts = [
                'yemek tarifi ver',
                'm√º≈üteri hizmet',
                'fiyatlar g√ºncellendi',
                'tamam, ≈üimdi',
                'sana yardƒ±m',
                'bir sonraki',
                'gerekli malzeme',
                'pi≈üirmek i√ßin'
            ]
            
            for bad_start in bad_starts:
                if lower_response.strip().startswith(bad_start):
                    return False
            
            # Check for repetitive company name mentions (sign of confusion)
            mefapex_count = lower_response.count('mefapex') + lower_response.count('mefopex') + lower_response.count('mefpex')
            if mefapex_count > 2:
                return False
            
            # Check for incomplete or fragmented sentences
            if response.count('...') > 2:
                return False
            
            # Check for HTML/URL fragments (sign of training data leakage)
            if any(fragment in lower_response for fragment in ['http', 'www', 'href', 'html', '.com']):
                return False
            
            # Check for excessive punctuation (sign of poor generation)
            punctuation_ratio = sum(1 for c in response if c in '.,!?;:') / max(len(response), 1)
            if punctuation_ratio > 0.15:
                return False
            
            # Check for date/time stamps (sign of training data)
            if any(fragment in response for fragment in ['2021', '2022', '2023', '2024', '2020', '2019']):
                return False
            
            # Check for meta-language about AI/chatbots
            if any(word in lower_response for word in ['ai', 'bot', 'asistan olarak', 'model olarak']):
                return False
            
            # Check for repeated words in sequence
            words = response.split()
            if len(words) >= 3:
                for i in range(len(words) - 2):
                    if words[i] == words[i+1] == words[i+2]:  # 3 consecutive identical words
                        return False
                        
            return True
            
        except Exception:
            return False
    
    def _is_response_relevant(self, question: str, response: str) -> bool:
        """
        Check if AI response is relevant to the original question
        """
        try:
            question_words = set(question.lower().split())
            response_words = set(response.lower().split())
            
            # Remove common stop words
            stop_words = {
                'bir', 'bu', '≈üu', 've', 'ile', 'i√ßin', 'den', 'dan', 'dƒ±r', 'dir',
                'mƒ±', 'mi', 'mu', 'm√º', 'da', 'de', 'ta', 'te', 'ya', 'ye',
                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with'
            }
            
            question_words -= stop_words
            response_words -= stop_words
            
            if not question_words:
                return True  # Can't determine, allow it
            
            # Check for word overlap
            overlap = len(question_words.intersection(response_words))
            overlap_ratio = overlap / len(question_words)
            
            # Require at least 10% word overlap for relevance
            return overlap_ratio >= 0.1
            
        except Exception:
            return True  # If can't determine, allow it

    def _calculate_match_score(self, user_message: str, keywords: List[str]) -> float:
        """Calculate keyword match score"""
        if not keywords:
            return 0.0
        
        user_words = set(user_message.lower().split())
        keyword_words = set()
        
        for keyword in keywords:
            keyword_words.update(keyword.lower().split())
        
        if not keyword_words:
            return 0.0
        
        # Calculate intersection score
        intersection = user_words.intersection(keyword_words)
        score = len(intersection) / len(keyword_words)
        
        # Bonus for exact phrase matches
        for keyword in keywords:
            if keyword.lower() in user_message.lower():
                score += 0.3
        
        return min(score, 1.0)

    def _get_intelligent_default_response(self, user_message: str = "") -> str:
        """
        Get contextual default response based on message content
        """
        if not user_message:
            return "Merhaba! Size nasƒ±l yardƒ±mcƒ± olabilirim?"
        
        message_lower = user_message.lower()
        
        # Context-aware responses based on message content
        if any(word in message_lower for word in ['hava', 'weather', 'sƒ±caklƒ±k', 'yaƒümur', 'kar', 'g√ºne≈ü']):
            return "üå§Ô∏è **Hava Durumu Sorgusu**\n\n√úzg√ºn√ºm, ger√ßek zamanlƒ± hava durumu bilgisine eri≈üimim yok. En g√ºncel hava durumu i√ßin:\n\n‚Ä¢ Meteoroloji Genel M√ºd√ºrl√ºƒü√º (mgm.gov.tr)\n‚Ä¢ Hava durumu uygulamalarƒ±\n‚Ä¢ Yerel haber kanallarƒ±\n\nkaynaklarƒ±nƒ± kullanabilirsiniz."
        
        elif any(word in message_lower for word in ['nasƒ±l', '√∂ƒüren', '√∂ƒüret', 'anlat', 'bilgi', 'nedir']):
            return "ü§î **Bilgi Talebi**\n\nSorduƒüunuz konu hakkƒ±nda detaylƒ± bilgi verebilmem i√ßin sorunuzu biraz daha spesifik hale getirebilir misiniz?\n\n**Yardƒ±mcƒ± olabileceƒüim konular:**\n‚Ä¢ MEFAPEX hizmetleri\n‚Ä¢ Teknik destek\n‚Ä¢ Yazƒ±lƒ±m geli≈ütirme\n‚Ä¢ Sistem entegrasyonu\n\nüí¨ Hangi konuda yardƒ±ma ihtiyacƒ±nƒ±z var?"
        
        elif any(word in message_lower for word in ['program', 'kod', 'yazƒ±lƒ±m', 'teknoloji', 'development']):
            return "üíª **Yazƒ±lƒ±m & Teknoloji**\n\nMEFAPEX olarak yazƒ±lƒ±m geli≈ütirme ve teknoloji konularƒ±nda size yardƒ±mcƒ± olabilirim.\n\n**Hizmetlerimiz:**\n‚Ä¢ √ñzel yazƒ±lƒ±m geli≈ütirme\n‚Ä¢ Web uygulamalarƒ±\n‚Ä¢ Mobil uygulamalar\n‚Ä¢ Sistem entegrasyonu\n‚Ä¢ Teknoloji danƒ±≈ümanlƒ±ƒüƒ±\n\nüîß Hangi teknoloji konusunda destek almak istiyorsunuz?"
        
        elif any(word in message_lower for word in ['problem', 'sorun', 'hata', '√ßalƒ±≈ümƒ±yor', 'bozuk']):
            return "üõ†Ô∏è **Teknik Destek**\n\nTeknik bir sorunla kar≈üƒ±la≈ütƒ±ƒüƒ±nƒ±zƒ± anlƒ±yorum. Size en iyi ≈üekilde yardƒ±mcƒ± olabilmem i√ßin:\n\n**L√ºtfen belirtin:**\n‚Ä¢ Hangi sistem/uygulama ile ilgili\n‚Ä¢ Sorunun detaylarƒ±\n‚Ä¢ Ne zaman ba≈üladƒ±\n‚Ä¢ Hata mesajlarƒ± (varsa)\n\nüìû Acil durumlar i√ßin: **destek@mefapex.com**"
        
        elif any(word in message_lower for word in ['te≈üekk√ºr', 'saƒüol', 'thanks']):
            return "üòä **Rica Ederim!**\n\nSize yardƒ±mcƒ± olabildiysem √ßok mutluyum. Ba≈üka sorularƒ±nƒ±z olduƒüunda her zaman buradayƒ±m.\n\nüåü ƒ∞yi √ßalƒ±≈ümalar dilerim!"
        
        else:
            # Generic but helpful response
            responses = [
                "ü§ù **MEFAPEX Destek**\n\nSorduƒüunuz konu hakkƒ±nda size daha iyi yardƒ±mcƒ± olabilmem i√ßin biraz daha detay verebilir misiniz?\n\n**Pop√ºler konular:**\n‚Ä¢ Teknik destek\n‚Ä¢ Yazƒ±lƒ±m geli≈ütirme\n‚Ä¢ Sistem entegrasyonu\n‚Ä¢ Proje danƒ±≈ümanlƒ±ƒüƒ±",
                
                "üí¨ **Size Nasƒ±l Yardƒ±mcƒ± Olabilirim?**\n\nMEFAPEX ekibi olarak size destek olmaktan mutluluk duyarƒ±z.\n\n**Hizmet alanlarƒ±mƒ±z:**\n‚Ä¢ Bili≈üim teknolojileri\n‚Ä¢ Yazƒ±lƒ±m √ß√∂z√ºmleri\n‚Ä¢ Teknik danƒ±≈ümanlƒ±k\n\nüìù Sorunuzu daha spesifik olarak sorabilir misiniz?",
                
                "üîç **Daha Fazla Bilgi**\n\nBu konuda size yardƒ±mcƒ± olmak istiyorum ancak sorunuzu tam olarak anlayamadƒ±m.\n\n**Deneyebilirsiniz:**\n‚Ä¢ Sorunuzu farklƒ± kelimelerle ifade edin\n‚Ä¢ Daha spesifik bilgi verin\n‚Ä¢ Hangi alanda yardƒ±m istediƒüinizi belirtin\n\nüí° Ben buradayƒ±m!"
            ]
            
            # Select response based on message hash for consistency
            import hashlib
            hash_value = int(hashlib.md5(user_message.encode()).hexdigest()[:8], 16)
            return responses[hash_value % len(responses)]

    def _get_default_response(self, user_message: str = "") -> str:
        """Get simple default response (legacy method)"""
        return self._get_intelligent_default_response(user_message)

    def get_categories(self) -> Dict:
        """Get all response categories"""
        return self.categories

    def get_stats(self) -> Dict:
        """Get content manager statistics"""
        stats = {
            "static_responses": len(self.static_responses),
            "categories": len(self.categories),
            "cache_entries": len(self._cache),
            "cache_enabled": self._cache_enabled,
            "ai_enabled": self._ai_enabled,
            "huggingface_available": self.model_manager is not None,
            "relevance_detection_enabled": self._relevance_detection_enabled,
            "relevance_detector_available": RELEVANCE_DETECTOR_AVAILABLE
        }
        
        # Add relevance detection stats
        if self._relevance_detection_enabled and self.relevance_detector:
            stats["relevance_detection"] = {
                "enabled": True,
                "company_name": self.relevance_detector.company_name,
                "domain_categories": len(self.relevance_detector.domain_categories),
                "quick_filter_keywords": {
                    "relevant": len(self.relevance_detector.quick_filters["definitely_relevant"]),
                    "irrelevant": len(self.relevance_detector.quick_filters["definitely_irrelevant"])
                }
            }
        else:
            stats["relevance_detection"] = {"enabled": False}
        
        # Add AI model stats if available
        if self.model_manager:
            try:
                model_info = self.model_manager.get_model_info()
                stats["ai_model_info"] = {
                    "turkish_model_loaded": model_info.get("turkish_sentence_model_loaded", False),
                    "text_generator_loaded": model_info.get("text_generator_loaded", False),
                    "device": model_info.get("device", "unknown"),
                    "cache_hits": model_info.get("cache_info", {}).get("embedding_cache_hits", 0),
                    "cache_size": model_info.get("cache_info", {}).get("embedding_cache_size", 0)
                }
            except Exception as e:
                logger.warning(f"Could not get AI model stats: {e}")
                stats["ai_model_info"] = {"error": str(e)}
        
        return stats

    def clear_cache(self):
        """Clear response cache"""
        self._cache.clear()
        logger.info("üóëÔ∏è Response cache cleared")

    def reload_content(self):
        """Reload static content and clear cache"""
        self.clear_cache()
        self.load_static_content()
        logger.info("üîÑ Content reloaded")
    
    async def find_response_async(self, user_message: str) -> Tuple[str, str]:
        """
        Async version of find_response for better performance with AI models
        """
        return self.find_response(user_message)
    
    def warmup_ai_models(self):
        """
        Warm up AI models for better response times
        """
        if self._ai_enabled and self.model_manager:
            try:
                logger.info("üî• Warming up AI models...")
                self.model_manager.warmup_models()
                logger.info("‚úÖ AI models warmed up successfully")
            except Exception as e:
                logger.error(f"‚ùå AI warmup failed: {e}")
        else:
            logger.info("‚ÑπÔ∏è AI models not available for warmup")
    
    def enable_ai(self, enabled: bool = True):
        """
        Enable or disable AI responses
        """
        if enabled and not self.model_manager:
            logger.warning("‚ö†Ô∏è Cannot enable AI: model_manager not available")
            return False
        
        self._ai_enabled = enabled
        status = "enabled" if enabled else "disabled"
        logger.info(f"ü§ñ AI responses {status}")
        return True
    
    def enable_relevance_detection(self, enabled: bool = True):
        """
        Enable or disable relevance detection
        """
        if enabled and not self.relevance_detector:
            logger.warning("‚ö†Ô∏è Cannot enable relevance detection: detector not available")
            return False
        
        self._relevance_detection_enabled = enabled
        status = "enabled" if enabled else "disabled"
        logger.info(f"üéØ Relevance detection {status}")
        return True
    
    async def test_relevance_detection(self, test_messages: List[str] = None):
        """
        Test relevance detection with sample messages
        """
        if not self._relevance_detection_enabled or not self.relevance_detector:
            logger.warning("‚ö†Ô∏è Relevance detection not available for testing")
            return {}
        
        if test_messages is None:
            test_messages = [
                "MEFAPEX fabrikasƒ±nda √ßalƒ±≈üma saatleri nedir?",  # Relevant
                "En iyi pizza tarifi nedir?",                    # Irrelevant
                "Yazƒ±lƒ±m geli≈ütirme hizmetleri",                 # Relevant
                "Hangi film izlemeliyim?",                       # Irrelevant
                "Teknik destek nasƒ±l alabilirim?",               # Relevant
                "Bug√ºn hava nasƒ±l?",                             # Irrelevant
            ]
        
        results = {}
        total_time = 0
        
        logger.info("üß™ Testing relevance detection...")
        
        for message in test_messages:
            try:
                result = await self.relevance_detector.classify(message)
                
                results[message] = {
                    "is_relevant": result.is_relevant,
                    "confidence": result.confidence,
                    "level": result.relevance_level.value,
                    "method": result.method.value,
                    "processing_time_ms": result.processing_time_ms,
                    "reasoning": result.reasoning,
                    "has_suggestion": result.response_suggestion is not None
                }
                
                total_time += result.processing_time_ms
                
                # Log result
                relevance_emoji = "‚úÖ" if result.is_relevant else "‚ùå"
                logger.info(f"{relevance_emoji} {message[:40]}... -> "
                          f"{result.confidence:.3f} ({result.processing_time_ms:.1f}ms)")
                
            except Exception as e:
                logger.error(f"‚ùå Test failed for '{message}': {e}")
                results[message] = {"error": str(e)}
        
        # Summary
        avg_time = total_time / len(test_messages) if test_messages else 0
        relevant_count = sum(1 for r in results.values() 
                           if isinstance(r, dict) and r.get("is_relevant", False))
        
        summary = {
            "total_tests": len(test_messages),
            "relevant_detected": relevant_count,
            "irrelevant_detected": len(test_messages) - relevant_count,
            "average_processing_time_ms": avg_time,
            "total_processing_time_ms": total_time
        }
        
        logger.info(f"üìä Test Summary: {relevant_count}/{len(test_messages)} relevant, "
                   f"avg time: {avg_time:.1f}ms")
        
        return {
            "results": results,
            "summary": summary
        }

# Global instance
content_manager = ContentManager()
