"""
ğŸ“Š Performance Evaluator for MEFAPEX AI Response Optimization
===========================================================
Utility to benchmark and compare AI response generation performance
before and after optimizations.
"""

import asyncio
import time
import statistics
import json
from typing import List, Dict, Any
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class PerformanceEvaluator:
    """Evaluate and compare AI response generation performance"""
    
    def __init__(self):
        self.results = []
        self.baseline_results = []
    
    async def benchmark_ai_response(self, test_messages: List[str], iterations: int = 5) -> Dict[str, Any]:
        """
        Benchmark AI response generation with test messages
        
        Args:
            test_messages: List of test messages to evaluate
            iterations: Number of iterations per message
            
        Returns:
            Performance benchmark results
        """
        try:
            # Import the optimized generate_ai_response function
            from api.chat import generate_ai_response
            
            logger.info(f"ğŸš€ Starting performance benchmark with {len(test_messages)} messages, {iterations} iterations each")
            
            benchmark_results = {
                'test_config': {
                    'message_count': len(test_messages),
                    'iterations_per_message': iterations,
                    'total_tests': len(test_messages) * iterations,
                    'timestamp': datetime.utcnow().isoformat()
                },
                'individual_results': [],
                'aggregate_metrics': {}
            }
            
            all_response_times = []
            source_counts = {}
            
            for msg_idx, message in enumerate(test_messages):
                logger.info(f"ğŸ“ Testing message {msg_idx + 1}/{len(test_messages)}: '{message[:50]}...'")
                
                message_results = {
                    'message': message,
                    'message_index': msg_idx,
                    'iterations': [],
                    'avg_response_time': 0,
                    'min_response_time': float('inf'),
                    'max_response_time': 0,
                    'sources_used': {}
                }
                
                for iteration in range(iterations):
                    start_time = time.time()
                    
                    try:
                        response, source = await generate_ai_response(message)
                        response_time_ms = int((time.time() - start_time) * 1000)
                        
                        # Record results
                        iteration_result = {
                            'iteration': iteration + 1,
                            'response_time_ms': response_time_ms,
                            'source': source,
                            'response_length': len(response) if response else 0,
                            'success': True
                        }
                        
                        message_results['iterations'].append(iteration_result)
                        all_response_times.append(response_time_ms)
                        
                        # Count sources
                        source_counts[source] = source_counts.get(source, 0) + 1
                        message_results['sources_used'][source] = message_results['sources_used'].get(source, 0) + 1
                        
                        # Update min/max times
                        message_results['min_response_time'] = min(message_results['min_response_time'], response_time_ms)
                        message_results['max_response_time'] = max(message_results['max_response_time'], response_time_ms)
                        
                        logger.debug(f"  Iteration {iteration + 1}: {response_time_ms}ms from {source}")
                        
                    except Exception as e:
                        logger.error(f"  Iteration {iteration + 1} failed: {e}")
                        iteration_result = {
                            'iteration': iteration + 1,
                            'response_time_ms': 0,
                            'source': 'error',
                            'response_length': 0,
                            'success': False,
                            'error': str(e)
                        }
                        message_results['iterations'].append(iteration_result)
                
                # Calculate message-level metrics
                successful_times = [r['response_time_ms'] for r in message_results['iterations'] if r['success']]
                if successful_times:
                    message_results['avg_response_time'] = statistics.mean(successful_times)
                    message_results['median_response_time'] = statistics.median(successful_times)
                    message_results['success_rate'] = len(successful_times) / len(message_results['iterations'])
                else:
                    message_results['avg_response_time'] = 0
                    message_results['median_response_time'] = 0
                    message_results['success_rate'] = 0
                
                benchmark_results['individual_results'].append(message_results)
            
            # Calculate aggregate metrics
            if all_response_times:
                benchmark_results['aggregate_metrics'] = {
                    'total_requests': len(all_response_times),
                    'avg_response_time_ms': statistics.mean(all_response_times),
                    'median_response_time_ms': statistics.median(all_response_times),
                    'min_response_time_ms': min(all_response_times),
                    'max_response_time_ms': max(all_response_times),
                    'std_dev_ms': statistics.stdev(all_response_times) if len(all_response_times) > 1 else 0,
                    'p95_response_time_ms': statistics.quantiles(all_response_times, n=20)[18] if len(all_response_times) >= 20 else max(all_response_times),
                    'p99_response_time_ms': statistics.quantiles(all_response_times, n=100)[98] if len(all_response_times) >= 100 else max(all_response_times),
                    'source_distribution': source_counts,
                    'optimization_indicators': {
                        'config_cached': True,  # Always true with our optimization
                        'parallel_processing': 'knowledge_base' in source_counts and 'static_content' in source_counts,
                        'early_return_used': any('knowledge_base' in str(r) for r in benchmark_results['individual_results'])
                    }
                }
            
            logger.info(f"âœ… Benchmark completed: {len(all_response_times)} successful requests")
            logger.info(f"ğŸ“Š Average response time: {benchmark_results['aggregate_metrics'].get('avg_response_time_ms', 0):.1f}ms")
            
            return benchmark_results
            
        except Exception as e:
            logger.error(f"Benchmark failed: {e}")
            raise
    
    def compare_performance(self, baseline_results: Dict[str, Any], optimized_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Compare performance between baseline and optimized implementations
        
        Args:
            baseline_results: Benchmark results from baseline implementation
            optimized_results: Benchmark results from optimized implementation
            
        Returns:
            Comparison analysis
        """
        try:
            baseline_metrics = baseline_results.get('aggregate_metrics', {})
            optimized_metrics = optimized_results.get('aggregate_metrics', {})
            
            if not baseline_metrics or not optimized_metrics:
                return {"error": "Invalid benchmark results for comparison"}
            
            comparison = {
                'comparison_timestamp': datetime.utcnow().isoformat(),
                'baseline_summary': {
                    'avg_response_time_ms': baseline_metrics.get('avg_response_time_ms', 0),
                    'median_response_time_ms': baseline_metrics.get('median_response_time_ms', 0),
                    'p95_response_time_ms': baseline_metrics.get('p95_response_time_ms', 0),
                    'total_requests': baseline_metrics.get('total_requests', 0)
                },
                'optimized_summary': {
                    'avg_response_time_ms': optimized_metrics.get('avg_response_time_ms', 0),
                    'median_response_time_ms': optimized_metrics.get('median_response_time_ms', 0),
                    'p95_response_time_ms': optimized_metrics.get('p95_response_time_ms', 0),
                    'total_requests': optimized_metrics.get('total_requests', 0)
                },
                'improvements': {},
                'optimization_benefits': {}
            }
            
            # Calculate improvements
            baseline_avg = baseline_metrics.get('avg_response_time_ms', 1)
            optimized_avg = optimized_metrics.get('avg_response_time_ms', 1)
            
            if baseline_avg > 0:
                comparison['improvements'] = {
                    'avg_response_time_improvement_percent': ((baseline_avg - optimized_avg) / baseline_avg) * 100,
                    'avg_response_time_reduction_ms': baseline_avg - optimized_avg,
                    'median_improvement_percent': ((baseline_metrics.get('median_response_time_ms', 1) - optimized_metrics.get('median_response_time_ms', 1)) / baseline_metrics.get('median_response_time_ms', 1)) * 100,
                    'p95_improvement_percent': ((baseline_metrics.get('p95_response_time_ms', 1) - optimized_metrics.get('p95_response_time_ms', 1)) / baseline_metrics.get('p95_response_time_ms', 1)) * 100
                }
            
            # Analyze optimization benefits
            optimized_indicators = optimized_metrics.get('optimization_indicators', {})
            comparison['optimization_benefits'] = {
                'config_caching_enabled': optimized_indicators.get('config_cached', False),
                'parallel_processing_detected': optimized_indicators.get('parallel_processing', False),
                'early_return_used': optimized_indicators.get('early_return_used', False),
                'source_distribution_optimized': len(optimized_metrics.get('source_distribution', {})) > 1
            }
            
            # Performance verdict
            avg_improvement = comparison['improvements'].get('avg_response_time_improvement_percent', 0)
            if avg_improvement > 20:
                comparison['verdict'] = "ğŸš€ Significant performance improvement achieved"
            elif avg_improvement > 10:
                comparison['verdict'] = "âœ… Good performance improvement achieved"
            elif avg_improvement > 0:
                comparison['verdict'] = "ğŸ” Minor performance improvement achieved"
            else:
                comparison['verdict'] = "âš ï¸ No significant performance improvement detected"
            
            logger.info(f"ğŸ“Š Performance comparison completed: {avg_improvement:.1f}% improvement")
            
            return comparison
            
        except Exception as e:
            logger.error(f"Performance comparison failed: {e}")
            return {"error": str(e)}
    
    def save_results(self, results: Dict[str, Any], filename: str):
        """Save benchmark results to file"""
        try:
            with open(filename, 'w') as f:
                json.dump(results, f, indent=2)
            logger.info(f"ğŸ“ Results saved to {filename}")
        except Exception as e:
            logger.error(f"Failed to save results: {e}")
    
    def load_results(self, filename: str) -> Dict[str, Any]:
        """Load benchmark results from file"""
        try:
            with open(filename, 'r') as f:
                results = json.load(f)
            logger.info(f"ğŸ“ Results loaded from {filename}")
            return results
        except Exception as e:
            logger.error(f"Failed to load results: {e}")
            return {}

# Sample test messages for benchmarking
SAMPLE_TEST_MESSAGES = [
    "Merhaba, nasÄ±lsÄ±nÄ±z?",
    "MEFAPEX Ã§alÄ±ÅŸma saatleri nedir?",
    "Teknik destek iÃ§in kimle iletiÅŸim kurmalÄ±yÄ±m?",
    "Åifre sÄ±fÄ±rlama iÅŸlemi nasÄ±l yapÄ±lÄ±r?",
    "Sistemde bir hata var, yardÄ±m edebilir misiniz?",
    "Ä°nsan kaynaklarÄ± departmanÄ±nÄ±n telefon numarasÄ± nedir?",
    "GÃ¼venlik kurallarÄ± hakkÄ±nda bilgi alabilir miyim?",
    "Proje durumu nedir?",
    "BugÃ¼n hava nasÄ±l?",  # Irrelevant question to test fallback
    "Bu konuda daha detaylÄ± bilgi verebilir misiniz?"
]

async def run_quick_benchmark():
    """Run a quick performance benchmark"""
    evaluator = PerformanceEvaluator()
    
    logger.info("ğŸš€ Running quick performance benchmark...")
    
    # Use a subset of test messages for quick testing
    quick_messages = SAMPLE_TEST_MESSAGES[:5]
    
    results = await evaluator.benchmark_ai_response(quick_messages, iterations=3)
    
    print("\n" + "="*60)
    print("ğŸ† QUICK BENCHMARK RESULTS")
    print("="*60)
    
    if results.get('aggregate_metrics'):
        metrics = results['aggregate_metrics']
        print(f"ğŸ“Š Average Response Time: {metrics.get('avg_response_time_ms', 0):.1f}ms")
        print(f"ğŸ“Š Median Response Time: {metrics.get('median_response_time_ms', 0):.1f}ms")
        print(f"ğŸ“Š 95th Percentile: {metrics.get('p95_response_time_ms', 0):.1f}ms")
        print(f"ğŸ“Š Total Requests: {metrics.get('total_requests', 0)}")
        print(f"ğŸ“Š Source Distribution: {metrics.get('source_distribution', {})}")
        print(f"ğŸš€ Optimization Status: {metrics.get('optimization_indicators', {})}")
    
    return results

if __name__ == "__main__":
    # Run quick benchmark
    asyncio.run(run_quick_benchmark())
