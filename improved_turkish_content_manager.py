"""
ğŸ‡¹ğŸ‡· GeliÅŸmiÅŸ TÃ¼rkÃ§e Content Manager
==================================
Daha iyi TÃ¼rkÃ§e yanÄ±tlar iÃ§in optimize edilmiÅŸ content yÃ¶neticisi
Morfological analysis ve lemmatization ile desteklenmiÅŸ

ğŸš€ ULTRA PERFORMANCE OPTIMIZATION:
- Morphological analyzer ARTIK TAMAMEN LAZY LOADING ile yÃ¼klenir
- EÅŸ anlamlÄ± kelimeler sadece kullanÄ±cÄ± soru sorduÄŸunda geniÅŸletilir  
- BaÅŸlangÄ±Ã§ zamanÄ± maksimum seviyede azaltÄ±ldÄ±
- Kaynak tÃ¼ketimi minimize edildi
- Zeyrek analyzer tamamen devre dÄ±ÅŸÄ± bÄ±rakÄ±ldÄ±

ğŸ”§ STRICT LAZY LOADING FEATURES:
- TurkishMorphAnalyzer: Sadece gerÃ§ek ihtiyaÃ§ anÄ±nda yÃ¼klenir
- Synonym expansion: Ä°lk soru iÅŸlendiÄŸinde aktif olur
- Morphological enhancement: Tamamen on-demand olarak Ã§alÄ±ÅŸÄ±r
- HiÃ§bir NLP tool otomatik olarak baÅŸlatÄ±lmaz

ğŸ”‡ SILENT MODE OPTIMIZATION:
- VarsayÄ±lan log seviyesi ERROR olarak ayarlandÄ±
- AyrÄ±ntÄ±lÄ± loglar sadece enable_detailed_logging=True ile gÃ¶rÃ¼nÃ¼r
- Gereksiz Ã§Ä±ktÄ±lar tamamen elimine edildi
- Sessiz Ã§alÄ±ÅŸma modu aktif
"""
import json
import re
import logging
import os
from typing import Dict, List, Tuple, Optional, Set
from difflib import SequenceMatcher

# Turkish NLP dependencies
try:
    import spacy
    from spacy.cli import download
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    spacy = None

# Alternative Turkish NLP libraries
try:
    from TurkishStemmer import TurkishStemmer
    TURKISH_STEMMER_AVAILABLE = True
except ImportError:
    TURKISH_STEMMER_AVAILABLE = False
    TurkishStemmer = None

# Zeyrek library - DISABLED due to verbose output and slow initialization
# ZEYREK COMPLETELY DISABLED TO PREVENT VERBOSE OUTPUT
ZEYREK_AVAILABLE = False
zeyrek = None

logger = logging.getLogger(__name__)

# Set default logging level to ERROR to minimize verbose output
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setLevel(logging.ERROR)
    formatter = logging.Formatter('%(levelname)s: %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.ERROR)
    logger.propagate = False

class TurkishMorphAnalyzer:
    """
    TÃ¼rkÃ§e morfological analysis ve lemmatization sÄ±nÄ±fÄ±
    spaCy Turkish model kullanÄ±r
    """
    
    def __init__(self, enable_detailed_logging=False):
        self.nlp = None
        self.turkish_stemmer = None
        self.zeyrek_analyzer = None
        self.fallback_lemmas = self._load_fallback_lemmas()
        self.enable_detailed_logging = enable_detailed_logging
        self._lazy_init = False  # Lazy initialization iÃ§in
        
        # Asla otomatik olarak initialize etme - sadece gerÃ§ekten gerektiÄŸinde
        # Eski kod: if enable_detailed_logging: self._initialize_nlp_tools()
        # Bu satÄ±r kaldÄ±rÄ±ldÄ± - artÄ±k hiÃ§bir durumda otomatik init yok
    
    def _ensure_initialized(self):
        """Lazy initialization - sadece gerektiÄŸinde NLP tools'larÄ± yÃ¼kle"""
        if not self._lazy_init:
            self._initialize_nlp_tools()
            self._lazy_init = True
    
    def _initialize_nlp_tools(self):
        """Initialize Turkish NLP tools with fallback options - ONLY when really needed"""
        # Sadece detailed logging etkinse debug mesajlarÄ± gÃ¶ster
        if self.enable_detailed_logging:
            print("ğŸ”§ Initializing Turkish NLP tools...")
        
        # Try to initialize TurkishStemmer
        if TURKISH_STEMMER_AVAILABLE:
            try:
                self.turkish_stemmer = TurkishStemmer()
                if self.enable_detailed_logging:
                    print("âœ… TurkishStemmer initialized successfully")
            except Exception as e:
                if self.enable_detailed_logging:
                    print(f"âš ï¸ Failed to initialize TurkishStemmer: {e}")
                
        # Try to initialize Zeyrek morphological analyzer - DISABLED to prevent verbose output
        # if ZEYREK_AVAILABLE:
        #     try:
        #         self.zeyrek_analyzer = zeyrek.MorphAnalyzer()
        #         if self.enable_detailed_logging:
        #             print("âœ… Zeyrek morphological analyzer initialized successfully")
        #     except Exception as e:
        #         if self.enable_detailed_logging:
        #             print(f"âš ï¸ Failed to initialize Zeyrek: {e}")
        
        # Try to initialize spaCy as backup
        if SPACY_AVAILABLE:
            try:
                # Try to load Turkish model
                try:
                    self.nlp = spacy.load("tr_core_news_sm")
                    if self.enable_detailed_logging:
                        print("âœ… Turkish spaCy model loaded successfully")
                except OSError:
                    # Model not found, try basic Turkish language support
                    if self.enable_detailed_logging:
                        print("âš ï¸ Turkish spaCy model (tr_core_news_sm) not found.")
                        print("ğŸ”„ Using basic Turkish language support")
                    try:
                        from spacy.lang.tr import Turkish
                        self.nlp = Turkish()
                        if self.enable_detailed_logging:
                            print("âœ… Basic Turkish language support loaded")
                    except Exception as fallback_error:
                        if self.enable_detailed_logging:
                            print(f"âš ï¸ Could not load basic Turkish support: {fallback_error}")
                        self.nlp = None
            except Exception as e:
                if self.enable_detailed_logging:
                    print(f"âŒ Error initializing spaCy: {e}")
                self.nlp = None
        
        # Report available tools - only if detailed logging enabled
        if self.enable_detailed_logging:
            available_tools = []
            if self.turkish_stemmer:
                available_tools.append("TurkishStemmer")
            if self.zeyrek_analyzer:
                available_tools.append("Zeyrek")
            if self.nlp:
                available_tools.append("spaCy")
            if available_tools:
                print(f"âœ… Available Turkish NLP tools: {', '.join(available_tools)}")
            else:
                print("ğŸ”„ Using fallback morphological analysis with manual lemma dictionary")
                print("ğŸ’¡ To enhance Turkish analysis, install: pip install TurkishStemmer zeyrek")
    
    def _load_fallback_lemmas(self) -> Dict[str, str]:
        """Fallback lemma sÃ¶zlÃ¼ÄŸÃ¼ yÃ¼kle"""
        return {
            # Fiiller (Verbs)
            'Ã§alÄ±ÅŸÄ±yor': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸÄ±yorum': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸÄ±yorsun': 'Ã§alÄ±ÅŸ',
            'Ã§alÄ±ÅŸan': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸmak': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸtÄ±': 'Ã§alÄ±ÅŸ',
            'gidiyor': 'git', 'gidiyorum': 'git', 'gitti': 'git', 'gitmek': 'git',
            'geliyor': 'gel', 'geliyorum': 'gel', 'geldi': 'gel', 'gelmek': 'gel',
            'yapÄ±yor': 'yap', 'yapÄ±yorum': 'yap', 'yaptÄ±': 'yap', 'yapmak': 'yap',
            'oluyor': 'ol', 'oluyorum': 'ol', 'oldu': 'ol', 'olmak': 'ol',
            'istiyor': 'iste', 'istiyorum': 'iste', 'istedi': 'iste', 'istemek': 'iste',
            'baÅŸlÄ±yor': 'baÅŸla', 'baÅŸlÄ±yorum': 'baÅŸla', 'baÅŸladÄ±': 'baÅŸla', 'baÅŸlamak': 'baÅŸla',
            'bitiyor': 'bit', 'bitiyorum': 'bit', 'bitti': 'bit', 'bitmek': 'bit',
            'aÃ§Ä±yor': 'aÃ§', 'aÃ§Ä±yorum': 'aÃ§', 'aÃ§tÄ±': 'aÃ§', 'aÃ§mak': 'aÃ§',
            'kapÄ±yor': 'kapa', 'kapÄ±yorum': 'kapa', 'kapattÄ±': 'kapa', 'kapamak': 'kapa',
            'alÄ±yor': 'al', 'alÄ±yorum': 'al', 'aldÄ±': 'al', 'almak': 'al',
            'veriyor': 'ver', 'veriyorum': 'ver', 'verdi': 'ver', 'vermek': 'ver',
            'buluyor': 'bul', 'buluyorum': 'bul', 'buldu': 'bul', 'bulmak': 'bul',
            'dÃ¼ÅŸÃ¼nÃ¼yor': 'dÃ¼ÅŸÃ¼n', 'dÃ¼ÅŸÃ¼nÃ¼yorum': 'dÃ¼ÅŸÃ¼n', 'dÃ¼ÅŸÃ¼ndÃ¼': 'dÃ¼ÅŸÃ¼n',
            'konuÅŸuyor': 'konuÅŸ', 'konuÅŸuyorum': 'konuÅŸ', 'konuÅŸtu': 'konuÅŸ',
            'Ã§alÄ±ÅŸmasÄ±': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸmasÄ±nÄ±': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸmasÄ±na': 'Ã§alÄ±ÅŸ',
            
            # Ä°simler (Nouns) - Ã‡oÄŸul ve hal ekleri
            'saatleri': 'saat', 'saatler': 'saat', 'saatin': 'saat', 'saate': 'saat',
            'gÃ¼nleri': 'gÃ¼n', 'gÃ¼nler': 'gÃ¼n', 'gÃ¼nÃ¼n': 'gÃ¼n', 'gÃ¼ne': 'gÃ¼n',
            'iÅŸleri': 'iÅŸ', 'iÅŸler': 'iÅŸ', 'iÅŸin': 'iÅŸ', 'iÅŸe': 'iÅŸ',
            'sorunlarÄ±': 'sorun', 'sorunlar': 'sorun', 'sorunun': 'sorun', 'soruna': 'sorun',
            'hatalarÄ±': 'hata', 'hatalar': 'hata', 'hatanÄ±n': 'hata', 'hataya': 'hata',
            'sistemleri': 'sistem', 'sistemler': 'sistem', 'sistemin': 'sistem', 'sisteme': 'sistem',
            'bilgileri': 'bilgi', 'bilgiler': 'bilgi', 'bilginin': 'bilgi', 'bilgiye': 'bilgi',
            'desteÄŸi': 'destek', 'destekler': 'destek', 'desteÄŸin': 'destek', 'desteÄŸe': 'destek',
            'gÃ¼venliÄŸi': 'gÃ¼venlik', 'gÃ¼venlikler': 'gÃ¼venlik', 'gÃ¼venliÄŸin': 'gÃ¼venlik',
            'personeli': 'personel', 'personeller': 'personel', 'personelin': 'personel',
            'projesi': 'proje', 'projeler': 'proje', 'projenin': 'proje', 'projeye': 'proje',
            'eÄŸitimi': 'eÄŸitim', 'eÄŸitimler': 'eÄŸitim', 'eÄŸitimin': 'eÄŸitim', 'eÄŸitime': 'eÄŸitim',
            'kullanÄ±cÄ±sÄ±': 'kullanÄ±cÄ±', 'kullanÄ±cÄ±lar': 'kullanÄ±cÄ±', 'kullanÄ±cÄ±nÄ±n': 'kullanÄ±cÄ±',
            
            # SÄ±fatlar (Adjectives)
            'iyi': 'iyi', 'iyiler': 'iyi', 'iyidir': 'iyi', 'iyiyi': 'iyi',
            'kÃ¶tÃ¼': 'kÃ¶tÃ¼', 'kÃ¶tÃ¼ler': 'kÃ¶tÃ¼', 'kÃ¶tÃ¼dÃ¼r': 'kÃ¶tÃ¼', 'kÃ¶tÃ¼yÃ¼': 'kÃ¶tÃ¼',
            'hÄ±zlÄ±': 'hÄ±zlÄ±', 'hÄ±zlÄ±lar': 'hÄ±zlÄ±', 'hÄ±zlÄ±dÄ±r': 'hÄ±zlÄ±',
            'yavaÅŸ': 'yavaÅŸ', 'yavaÅŸlar': 'yavaÅŸ', 'yavaÅŸtÄ±r': 'yavaÅŸ',
            'kolay': 'kolay', 'kolaylar': 'kolay', 'kolaydÄ±r': 'kolay',
            'zor': 'zor', 'zorlar': 'zor', 'zordur': 'zor',
            'yeni': 'yeni', 'yeniler': 'yeni', 'yenidir': 'yeni',
            'eski': 'eski', 'eskiler': 'eski', 'eskidir': 'eski',
            'bÃ¼yÃ¼k': 'bÃ¼yÃ¼k', 'bÃ¼yÃ¼kler': 'bÃ¼yÃ¼k', 'bÃ¼yÃ¼ktÃ¼r': 'bÃ¼yÃ¼k',
            'kÃ¼Ã§Ã¼k': 'kÃ¼Ã§Ã¼k', 'kÃ¼Ã§Ã¼kler': 'kÃ¼Ã§Ã¼k', 'kÃ¼Ã§Ã¼ktÃ¼r': 'kÃ¼Ã§Ã¼k',
            
            # Zamanlar (Time expressions)
            'bugÃ¼n': 'bugÃ¼n', 'bugÃ¼nÃ¼': 'bugÃ¼n', 'bugÃ¼ne': 'bugÃ¼n',
            'yarÄ±n': 'yarÄ±n', 'yarÄ±nÄ±': 'yarÄ±n', 'yarÄ±na': 'yarÄ±n',
            'dÃ¼n': 'dÃ¼n', 'dÃ¼nÃ¼': 'dÃ¼n', 'dÃ¼ne': 'dÃ¼n',
            'ÅŸimdi': 'ÅŸimdi', 'ÅŸimdiyi': 'ÅŸimdi', 'ÅŸimdiye': 'ÅŸimdi',
            'sonra': 'sonra', 'sonrasÄ±': 'sonra', 'sonrasÄ±na': 'sonra',
            'Ã¶nce': 'Ã¶nce', 'Ã¶ncesi': 'Ã¶nce', 'Ã¶ncesine': 'Ã¶nce',
        }
    
    def lemmatize_word(self, word: str) -> str:
        """Tek kelimeyi lemmatize et using multiple Turkish NLP tools"""
        if not word:
            return word
        
        # Sadece gerektiÄŸinde initialize et
        self._ensure_initialized()
        
        word_lower = word.lower()
        
        # ZEYREK DISABLED to prevent verbose output
        # Try Zeyrek morphological analyzer first (most accurate) - DISABLED
        # if self.zeyrek_analyzer:
        #     try:
        #         analysis = self.zeyrek_analyzer.analyze(word_lower)
        #         if analysis and len(analysis) > 0:
        #             lemma = analysis[0][1].lemma
        #             if lemma and lemma != word_lower:
        #                 if self.enable_detailed_logging:
        #                     print(f"Zeyrek lemmatization: '{word}' â†’ '{lemma}'")
        #                 return lemma
        #     except Exception as e:
        #         if self.enable_detailed_logging:
        #             print(f"Zeyrek lemmatization error for '{word}': {e}")
        
        # Try TurkishStemmer (good for stemming)
        if self.turkish_stemmer:
            try:
                stem = self.turkish_stemmer.stem(word_lower)
                if stem and stem != word_lower and len(stem) >= 2:
                    if self.enable_detailed_logging:
                        print(f"TurkishStemmer: '{word}' â†’ '{stem}'")
                    return stem
            except Exception as e:
                if self.enable_detailed_logging:
                    print(f"TurkishStemmer error for '{word}': {e}")
        
        # Try spaCy (if available) - with minimal logging
        if self.nlp:
            try:
                doc = self.nlp(word_lower)
                if doc and len(doc) > 0:
                    lemma = doc[0].lemma_
                    if lemma and lemma != word_lower:
                        if self.enable_detailed_logging:
                            print(f"spaCy lemmatization: '{word}' â†’ '{lemma}'")
                        return lemma
            except Exception as e:
                if self.enable_detailed_logging:
                    print(f"spaCy lemmatization error for '{word}': {e}")
        
        # Fallback lemma sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ kullan
        if word_lower in self.fallback_lemmas:
            if self.enable_detailed_logging:
                print(f"Fallback lemma: '{word}' â†’ '{self.fallback_lemmas[word_lower]}'")
            return self.fallback_lemmas[word_lower]
        
        # Temel morfological rules
        basic_result = self._apply_basic_rules(word_lower)
        if basic_result != word_lower and self.enable_detailed_logging:
            print(f"Basic rules: '{word}' â†’ '{basic_result}'")
        return basic_result
    
    def _apply_basic_rules(self, word: str) -> str:
        """Temel morfological kurallarÄ± uygula"""
        if len(word) < 3:
            return word
        
        # Ã‡oÄŸul ekleri (-ler, -lar)
        if word.endswith(('ler', 'lar')):
            base = word[:-3]
            if len(base) >= 2:
                return base
        
        # Ä°yelik ekleri (-i, -Ä±, -u, -Ã¼)
        if word.endswith(('larÄ±', 'leri', 'larÄ±', 'leri')):
            base = word[:-4]
            if len(base) >= 2:
                return base
        
        # Hal ekleri
        if word.endswith(('nÄ±n', 'nin', 'nun', 'nÃ¼n')):
            base = word[:-3]
            if len(base) >= 2:
                return base
        
        if word.endswith(('na', 'ne', 'ya', 'ye')):
            base = word[:-2]
            if len(base) >= 2:
                return base
        
        # Fiil ekleri
        if word.endswith(('Ä±yor', 'iyor', 'uyor', 'Ã¼yor')):
            base = word[:-4]
            if len(base) >= 2:
                return base
        
        if word.endswith(('mak', 'mek')):
            base = word[:-3]
            if len(base) >= 2:
                return base
        
        return word
    
    def lemmatize_text(self, text: str) -> str:
        """Metindeki tÃ¼m kelimeleri lemmatize et"""
        if not text:
            return text
        
        words = text.split()
        lemmatized_words = []
        
        for word in words:
            # Noktalama iÅŸaretlerini ayÄ±r
            clean_word = re.sub(r'[^\w\s]', '', word)
            if clean_word:
                lemma = self.lemmatize_word(clean_word)
                lemmatized_words.append(lemma)
        
        return ' '.join(lemmatized_words)
    
    def get_word_variants(self, word: str) -> Set[str]:
        """Bir kelimenin farklÄ± varyantlarÄ±nÄ± Ã¼ret"""
        variants = {word.lower()}
        lemma = self.lemmatize_word(word)
        variants.add(lemma)
        
        # Temel varyantlar ekle
        if lemma in self.fallback_lemmas.values():
            # Bu lemma iÃ§in bilinen tÃ¼m formlarÄ± bul
            for inflected, base in self.fallback_lemmas.items():
                if base == lemma:
                    variants.add(inflected)
        
        return variants

logger = logging.getLogger(__name__)

class ImprovedTurkishContentManager:
    """
    GeliÅŸmiÅŸ TÃ¼rkÃ§e content yÃ¶neticisi
    - Daha kapsamlÄ± static responses
    - On-demand morfological analysis ile geliÅŸmiÅŸ matching
    - TÃ¼rkÃ§e karakter desteÄŸi
    - Lemmatization ile akÄ±llÄ± eÅŸleÅŸtirme
    - AkÄ±llÄ± fallback responses
    """
    
    def __init__(self, enable_detailed_logging=False):
        # Ä°lk Ã¶nce tÃ¼m Ã¶zellikleri ata
        self.enable_detailed_logging = enable_detailed_logging
        self.pattern_cache = {}
        
        # Lazy loading iÃ§in morphological analyzer'Ä± None olarak baÅŸlat
        self.morph_analyzer = None
        self.responses = self._load_enhanced_responses()
        
        # EÅŸ anlamlÄ± kelimeleri de lazy loading ile yÃ¼kle
        self._synonym_map = None
        self._synonyms_loaded = False
        self._synonyms_enhanced = False  # Morfological geniÅŸletme yapÄ±ldÄ± mÄ±?
        
        # Sadece detailed logging aktifse bu mesajÄ± gÃ¶ster
        if enable_detailed_logging:
            print("ğŸ‡¹ğŸ‡· Enhanced Turkish Content Manager with lazy morphological analysis initialized")
        # Sessiz mod - hiÃ§ log gÃ¶sterme
    
    @property
    def synonym_map(self):
        """Lazy loading property for synonym map"""
        if not self._synonyms_loaded:
            # Sadece detailed logging etkinse mesaj gÃ¶ster
            if self.enable_detailed_logging:
                print("ğŸ”§ Loading synonyms on-demand...")
            self._synonym_map = self._load_synonyms_from_file()
            self._synonyms_loaded = True
        return self._synonym_map
    
    def _get_morph_analyzer(self):
        """Lazy loading: Sadece gerektiÄŸinde morphological analyzer'Ä± oluÅŸtur"""
        if self.morph_analyzer is None:
            # Sadece detailed logging etkinse mesaj gÃ¶ster
            if self.enable_detailed_logging:
                print("ğŸ”§ Creating morphological analyzer on-demand...")
            self.morph_analyzer = TurkishMorphAnalyzer(enable_detailed_logging=self.enable_detailed_logging)
        return self.morph_analyzer
    
    def _load_synonyms_from_file(self) -> Dict[str, List[str]]:
        """synonyms.json dosyasÄ±ndan eÅŸ anlamlÄ± kelimeleri yÃ¼kle"""
        try:
            # Ã–nce mevcut dizinde ara
            synonyms_path = "content/synonyms.json"
            if not os.path.exists(synonyms_path):
                # Alternatif yollarÄ± dene
                possible_paths = [
                    "./content/synonyms.json",
                    "../content/synonyms.json",
                    "/Users/bariskose/Downloads/MefapexChatBox-main/content/synonyms.json"
                ]
                for path in possible_paths:
                    if os.path.exists(path):
                        synonyms_path = path
                        break
            
            with open(synonyms_path, 'r', encoding='utf-8') as f:
                loaded_synonyms = json.load(f)
            
            # Ä°lk yÃ¼klemede sadece basit synonyms'larÄ± kullan
            # Morfological geniÅŸletme ilk kullanÄ±mda yapÄ±lacak
            simple_synonyms = {}
            for base_word, synonyms in loaded_synonyms.items():
                # Sadece temel kelimeleri ekle, morfological analiz yapmadan
                all_variants = set()
                all_variants.add(base_word.lower())
                
                for synonym in synonyms:
                    all_variants.add(synonym.lower())
                
                # BoÅŸ stringleri filtrele
                all_variants = [v for v in all_variants if v and len(v) > 1]
                simple_synonyms[base_word.lower()] = list(all_variants)
            
            # Sadece detailed logging etkinse bu mesajÄ± gÃ¶ster
            if self.enable_detailed_logging:
                print(f"ğŸ“š Loaded {len(simple_synonyms)} synonym groups from file (basic mode)")
            return simple_synonyms
            
        except Exception as e:
            # Sadece detailed logging etkinse warning gÃ¶ster
            if self.enable_detailed_logging:
                print(f"âš ï¸ Could not load synonyms from file: {e}")
            return self._create_basic_synonym_map()  # Fallback to hardcoded synonyms
    
    def _create_basic_synonym_map(self) -> Dict[str, List[str]]:
        """Fallback: TÃ¼rkÃ§e eÅŸ anlamlÄ± kelimeler (hard-coded, basic mode)"""
        return {
            'Ã§alÄ±ÅŸ': ['iÅŸ', 'mesai', 'gÃ¶rev', 'vazife', 'work', 'job', 'Ã§alÄ±ÅŸma'],
            'saat': ['zaman', 'vakit', 'time', 'hours', 'hour'],
            'aÃ§': ['open', 'baÅŸla', 'start', 'begin', 'aÃ§Ä±k'],
            'kapa': ['closed', 'bit', 'end', 'finish', 'stop', 'kapalÄ±'],
            'destek': ['yardÄ±m', 'help', 'support', 'assistance', 'aid'],
            'sorun': ['problem', 'hata', 'error', 'issue', 'bug'],
            'mefapex': ['ÅŸirket', 'company', 'firma', 'organization', 'kurum'],
            'gÃ¼venlik': ['security', 'safety', 'emniyet', 'koruma'],
            'izin': ['leave', 'vacation', 'tatil', 'permit', 'permission'],
            'proje': ['project', 'gÃ¶rev', 'task', 'iÅŸ', 'work'],
            'eÄŸitim': ['training', 'education', 'kurs', 'course', 'Ã¶ÄŸretim'],
            'sistem': ['system', 'software', 'yazÄ±lÄ±m', 'program'],
            'hata': ['error', 'bug', 'problem', 'sorun', 'issue'],
            'yardÄ±m': ['help', 'support', 'destek', 'assistance']
        }
    
    def _load_enhanced_responses(self) -> Dict:
        """GeliÅŸmiÅŸ TÃ¼rkÃ§e yanÄ±tlar yÃ¼kle"""
        return {
            "greeting": {
                "patterns": [
                    "merhaba", "selam", "selamlar", "selamun aleykÃ¼m", "gÃ¼naydÄ±n", 
                    "iyi gÃ¼nler", "iyi akÅŸamlar", "iyi geceler", "nasÄ±lsÄ±n", 
                    "nasÄ±l gidiyor", "naber", "hello", "hi", "hey"
                ],
                "responses": [
                    "ğŸ™‹â€â™‚ï¸ **Merhaba! HoÅŸ geldiniz MEFAPEX'e!**\n\nBen sizin AI asistanÄ±nÄ±zÄ±m. Size ÅŸu konularda yardÄ±mcÄ± olabilirim:\n\nâ€¢ ğŸ­ **Fabrika OperasyonlarÄ±**: Ã‡alÄ±ÅŸma saatleri, vardiya bilgileri\nâ€¢ ğŸ’» **Teknik Destek**: YazÄ±lÄ±m, sistem ve IT sorunlarÄ±\nâ€¢ ğŸ‘¥ **Ä°nsan KaynaklarÄ±**: Ä°zin baÅŸvurularÄ±, personel iÅŸlemleri\nâ€¢ ğŸ›¡ï¸ **GÃ¼venlik**: Ä°ÅŸ gÃ¼venliÄŸi kurallarÄ± ve prosedÃ¼rler\nâ€¢ ğŸ¢ **Åirket Bilgileri**: MEFAPEX hizmetleri ve politikalarÄ±\n\nSorularÄ±nÄ±zÄ± bekliyorum! ğŸ˜Š",
                    
                    "ğŸ‘‹ **HoÅŸ geldiniz!**\n\nMEFAPEX BiliÅŸim Teknolojileri'nin AI asistanÄ±yÄ±m. Size nasÄ±l yardÄ±mcÄ± olabilirim?\n\n**PopÃ¼ler Sorular:**\nâ€¢ \"Ã‡alÄ±ÅŸma saatleri nelerdir?\"\nâ€¢ \"Teknik destek nasÄ±l alabilirim?\"\nâ€¢ \"Ä°zin baÅŸvurusu nasÄ±l yapÄ±lÄ±r?\"\nâ€¢ \"GÃ¼venlik kurallarÄ± nelerdir?\"\n\nDetaylÄ± bilgi iÃ§in sorunuzu yazabilirsiniz! ğŸš€"
                ]
            },
            
            "working_hours": {
                "patterns": [
                    "Ã§alÄ±ÅŸma saat", "iÅŸ saat", "mesai", "vardiya", "kaÃ§ta aÃ§Ä±k", 
                    "kaÃ§ta kapalÄ±", "ne zaman aÃ§Ä±k", "working hours", "office hours",
                    "Ã§alÄ±ÅŸma zamanÄ±", "iÅŸ zamanÄ±", "aÃ§Ä±lÄ±ÅŸ", "kapanÄ±ÅŸ", "ofis saat"
                ],
                "responses": [
                    "â° **MEFAPEX Ã‡alÄ±ÅŸma Saatleri**\n\n**ğŸ“… Hafta Ä°Ã§i Ã‡alÄ±ÅŸma:**\nâ€¢ Pazartesi - Cuma: 09:00 - 18:00\nâ€¢ Ã–ÄŸle molasÄ±: 12:00 - 13:00\n\n**ğŸ“… Hafta Sonu:**\nâ€¢ Cumartesi: 09:00 - 13:00 (YarÄ±m gÃ¼n)\nâ€¢ Pazar: KapalÄ±\n\n**ğŸš¨ Acil Durumlar:**\nâ€¢ 7/24 teknik destek hattÄ±\nâ€¢ Kritik sistem arÄ±zalarÄ± iÃ§in anÄ±nda mÃ¼dahale\n\n**ğŸ“ Ä°letiÅŸim:**\nâ€¢ Ana hat: [Telefon numarasÄ±]\nâ€¢ Acil: [Acil telefon]\nâ€¢ Email: info@mefapex.com",
                    
                    "ğŸ•˜ **Ä°ÅŸ Saatleri ve Vardiya Bilgileri**\n\n**Standart Mesai:**\nâ€¢ BaÅŸlangÄ±Ã§: 09:00\nâ€¢ BitiÅŸ: 18:00\nâ€¢ Toplam: 8 saat + 1 saat mola\n\n**Mola Saatleri:**\nâ€¢ Ã–ÄŸle arasÄ±: 12:00-13:00\nâ€¢ Ã‡ay molasÄ±: 10:00 ve 15:30\n\n**Esnek Ã‡alÄ±ÅŸma:**\nâ€¢ Uzaktan Ã§alÄ±ÅŸma imkanÄ±\nâ€¢ Esnek baÅŸlangÄ±Ã§ saatleri (08:00-10:00)\n\nDetaylÄ± bilgi iÃ§in Ä°K departmanÄ±yla iletiÅŸime geÃ§in."
                ]
            },
            
            "technical_support": {
                "patterns": [
                    "teknik destek", "technical support", "yardÄ±m", "help", "problem", 
                    "sorun", "hata", "error", "arÄ±za", "breakdown", "sistem", "yazÄ±lÄ±m",
                    "bilgisayar", "network", "aÄŸ", "server", "sunucu"
                ],
                "responses": [
                    "ğŸ› ï¸ **MEFAPEX Teknik Destek**\n\n**ğŸ“ AnÄ±nda Destek:**\nâ€¢ Teknik Destek HattÄ±: [Telefon]\nâ€¢ Email: destek@mefapex.com\nâ€¢ CanlÄ± Chat: Bu platform Ã¼zerinden\n\n**ğŸ”§ Destek TÃ¼rleri:**\nâ€¢ **YazÄ±lÄ±m DesteÄŸi**: Uygulama hatalarÄ±, gÃ¼ncelleme\nâ€¢ **Sistem DesteÄŸi**: Sunucu, aÄŸ altyapÄ±sÄ±\nâ€¢ **Acil MÃ¼dahale**: Kritik sistem Ã§Ã¶kmeleri\nâ€¢ **KullanÄ±cÄ± EÄŸitimi**: Sistem kullanÄ±m rehberi\n\n**â±ï¸ YanÄ±t SÃ¼releri:**\nâ€¢ Acil: 15 dakika\nâ€¢ Normal: 2 saat\nâ€¢ PlanlÄ±: 24 saat\n\nProbleminizi detaylandÄ±rÄ±n, hemen yardÄ±mcÄ± olalÄ±m! ğŸš€",
                    
                    "ğŸ’» **IT Destek ve Ã‡Ã¶zÃ¼mler**\n\n**HÄ±zlÄ± Ã‡Ã¶zÃ¼mler:**\nâ€¢ Åifre sÄ±fÄ±rlama: [Link]\nâ€¢ VPN kurulumu: [Rehber]\nâ€¢ Email ayarlarÄ±: [KÄ±lavuz]\n\n**Acil Durumlar:**\nâ€¢ Sistem Ã§Ã¶kmesi âœ Hemen arayÄ±n\nâ€¢ GÃ¼venlik ihlali âœ Acil bildirim\nâ€¢ Veri kaybÄ± âœ Backup kurtarma\n\n**ğŸ“± Uzaktan Destek:**\nâ€¢ TeamViewer baÄŸlantÄ±sÄ±\nâ€¢ Ekran paylaÅŸÄ±mÄ±\nâ€¢ CanlÄ± rehberlik\n\nSorununuzu aÃ§Ä±klayÄ±n, Ã§Ã¶zÃ¼m bulalÄ±m!"
                ]
            },
            
            "company_info": {
                "patterns": [
                    "mefapex", "ÅŸirket", "company", "firma", "hakkÄ±nda", "about", 
                    "biliÅŸim teknolojileri", "nedir", "ne yapÄ±yor", "hizmet",
                    "teknoloji", "yazÄ±lÄ±m geliÅŸtirme"
                ],
                "responses": [
                    "ğŸ¢ **MEFAPEX BiliÅŸim Teknolojileri**\n\n**ğŸ¯ Misyonumuz:**\nModern teknoloji Ã§Ã¶zÃ¼mleri ile iÅŸletmelerin dijital dÃ¶nÃ¼ÅŸÃ¼mÃ¼nde Ã¶ncÃ¼ olmak\n\n**ğŸ’¼ Hizmet AlanlarÄ±mÄ±z:**\nâ€¢ **ğŸ–¥ï¸ YazÄ±lÄ±m GeliÅŸtirme**: Web, mobil, masaÃ¼stÃ¼ uygulamalar\nâ€¢ **â˜ï¸ Bulut Ã‡Ã¶zÃ¼mleri**: AWS, Azure, Google Cloud\nâ€¢ **ğŸ”’ Siber GÃ¼venlik**: Penetrasyon testleri, gÃ¼venlik danÄ±ÅŸmanlÄ±ÄŸÄ±\nâ€¢ **ğŸ“Š Veri Analizi**: Big Data, Business Intelligence\nâ€¢ **ğŸ¤– Yapay Zeka**: Machine Learning, chatbot geliÅŸtirme\nâ€¢ **ğŸŒ Sistem Entegrasyonu**: ERP, CRM sistemleri\n\n**ğŸŒŸ Neden MEFAPEX?**\nâ€¢ 10+ yÄ±l tecrÃ¼be\nâ€¢ ISO 27001 sertifikalÄ±\nâ€¢ 7/24 destek\nâ€¢ YenilikÃ§i Ã§Ã¶zÃ¼mler",
                    
                    "ğŸš€ **MEFAPEX: Teknolojide GÃ¼venilir Ã‡Ã¶zÃ¼m OrtaÄŸÄ±nÄ±z**\n\n**ğŸ“ˆ BaÅŸarÄ± Hikayeleri:**\nâ€¢ 500+ tamamlanan proje\nâ€¢ %98 mÃ¼ÅŸteri memnuniyeti\nâ€¢ 50+ teknoloji uzmanÄ±\n\n**ğŸ† Sertifikalar:**\nâ€¢ ISO 9001 Kalite YÃ¶netimi\nâ€¢ ISO 27001 Bilgi GÃ¼venliÄŸi\nâ€¢ Microsoft Partner\nâ€¢ AWS Partner Network\n\n**ğŸ“ Ä°letiÅŸim:**\nâ€¢ Merkez: Ä°stanbul\nâ€¢ Åubeler: Ankara, Ä°zmir\nâ€¢ Web: www.mefapex.com\nâ€¢ Email: info@mefapex.com\n\nDaha detaylÄ± bilgi iÃ§in bizimle iletiÅŸime geÃ§in!"
                ]
            },
            
            "hr_processes": {
                "patterns": [
                    "izin", "leave", "tatil", "vacation", "baÅŸvuru", "application",
                    "personel", "ik", "human resources", "Ã¶zlÃ¼k", "bordro", "maaÅŸ"
                ],
                "responses": [
                    "ğŸ‘¥ **Ä°nsan KaynaklarÄ± SÃ¼reÃ§leri**\n\n**ğŸ“ Ä°zin BaÅŸvurularÄ±:**\nâ€¢ YÄ±llÄ±k izin: Online sistem Ã¼zerinden\nâ€¢ Mazeret izni: Ãœst amir onayÄ± ile\nâ€¢ HastalÄ±k izni: Rapor ibrazÄ±\n\n**ğŸ“‹ Gerekli Belgeler:**\nâ€¢ Ä°zin formu (doldurulmuÅŸ)\nâ€¢ Varsa tÄ±bbi rapor\nâ€¢ Ãœst amir onayÄ±\n\n**â±ï¸ BaÅŸvuru SÃ¼releri:**\nâ€¢ YÄ±llÄ±k izin: 15 gÃ¼n Ã¶nceden\nâ€¢ Acil izin: Amir onayÄ± ile\n\n**ğŸ“ Ä°K Ä°letiÅŸim:**\nâ€¢ Ä°K DepartmanÄ±: [Telefon]\nâ€¢ Email: ik@mefapex.com\nâ€¢ Ofis: Ana bina 2. kat\n\nDetaylÄ± bilgi iÃ§in Ä°K departmanÄ±mÄ±zla iletiÅŸime geÃ§in.",
                    
                    "ğŸ“Š **Personel Ä°ÅŸlemleri ve Haklar**\n\n**ğŸ’° MaaÅŸ ve Ã–demeler:**\nâ€¢ Bordro: Her ayÄ±n 30'u\nâ€¢ Prim Ã¶demeleri: Performansa gÃ¶re\nâ€¢ Yan haklar: Yemek, ulaÅŸÄ±m, saÄŸlÄ±k\n\n**ğŸ“ EÄŸitim ve GeliÅŸim:**\nâ€¢ Teknik kurslar\nâ€¢ Sertifikasyon programlarÄ±\nâ€¢ Konferans katÄ±lÄ±mlarÄ±\n\n**ğŸ¥ Sosyal Haklar:**\nâ€¢ Ã–zel saÄŸlÄ±k sigortasÄ±\nâ€¢ Yemek kartÄ±\nâ€¢ UlaÅŸÄ±m desteÄŸi\nâ€¢ Spor Ã¼yeliÄŸi\n\nKiÅŸisel dosyanÄ±z iÃ§in Ä°K'ya baÅŸvurun."
                ]
            },
            
            "security_safety": {
                "patterns": [
                    "gÃ¼venlik", "security", "safety", "kural", "rule", "kaza", "accident",
                    "acil", "emergency", "koruyucu", "protective", "kask", "helmet"
                ],
                "responses": [
                    "ğŸ›¡ï¸ **Ä°ÅŸ GÃ¼venliÄŸi KurallarÄ±**\n\n**âš ï¸ Zorunlu Koruyucu Ekipmanlar:**\nâ€¢ ğŸ¦º Reflektif yelek\nâ€¢ ğŸ‘· GÃ¼venlik kaskÄ±\nâ€¢ ğŸ‘Ÿ GÃ¼venlik ayakkabÄ±sÄ±\nâ€¢ ğŸ§¤ Ä°ÅŸ eldivenleri\nâ€¢ ğŸ¥½ Koruyucu gÃ¶zlÃ¼k\n\n**ğŸš¨ Acil Durum ProsedÃ¼rleri:**\nâ€¢ YangÄ±n: Alarm Ã§al, binayÄ± boÅŸalt\nâ€¢ Kaza: Ä°lk yardÄ±m, 112'yi ara\nâ€¢ GÃ¼venlik ihlali: GÃ¼venliÄŸi bilgilendir\n\n**ğŸ“ Acil Numaralar:**\nâ€¢ Ä°tfaiye: 110\nâ€¢ Ambulans: 112\nâ€¢ Polis: 155\nâ€¢ Ä°Ã§ hat gÃ¼venlik: [Dahili]\n\n**ğŸ“‹ GÃ¼venlik EÄŸitimleri:**\nâ€¢ AylÄ±k gÃ¼venlik toplantÄ±larÄ±\nâ€¢ Ä°lk yardÄ±m sertifikasÄ±\nâ€¢ YangÄ±n tatbikatlarÄ±",
                    
                    "ğŸ”’ **GÃ¼venlik Protokolleri ve Ã–nlemler**\n\n**ğŸ­ Fabrika GÃ¼venliÄŸi:**\nâ€¢ Makine Ã§alÄ±ÅŸÄ±rken yaklaÅŸma\nâ€¢ GÃ¼venlik bariyerlerine uy\nâ€¢ UyarÄ± levhalarÄ±nÄ± oku\nâ€¢ Temizlik maddelerini dikkatli kullan\n\n**ğŸ’» Bilgi GÃ¼venliÄŸi:**\nâ€¢ GÃ¼Ã§lÃ¼ ÅŸifreler kullan\nâ€¢ 2FA (iki faktÃ¶rlÃ¼ doÄŸrulama) aktif et\nâ€¢ ÅÃ¼pheli emailleri aÃ§ma\nâ€¢ USB cihazlarÄ± kontrol et\n\n**ğŸ†” EriÅŸim GÃ¼venliÄŸi:**\nâ€¢ KartÄ±nÄ± baÅŸkasÄ±na verme\nâ€¢ KapÄ±larÄ± arkandan kitle\nâ€¢ ZiyaretÃ§ileri kaydet\n\nGÃ¼venlik sizin sorumluluÄŸunuz!"
                ]
            },
            
            "thanks_goodbye": {
                "patterns": [
                    "teÅŸekkÃ¼r", "thanks", "saÄŸol", "thank you", "bye", "gÃ¶rÃ¼ÅŸÃ¼rÃ¼z",
                    "hoÅŸÃ§a kal", "goodbye", "elveda", "iyi gÃ¼nler"
                ],
                "responses": [
                    "ğŸ™ **Rica ederim!**\n\nSize yardÄ±mcÄ± olabildiysem ne mutlu bana! \n\n**ğŸ“ Ä°htiyaÃ§ halinde:**\nâ€¢ Teknik destek: destek@mefapex.com\nâ€¢ Genel bilgi: info@mefapex.com\nâ€¢ Acil durumlar: [Acil telefon]\n\nBaÅŸka sorularÄ±nÄ±z olduÄŸunda her zaman buradayÄ±m.\n\nâœ¨ **MEFAPEX ailesinin bir parÃ§asÄ± olduÄŸunuz iÃ§in teÅŸekkÃ¼rler!**\n\nÄ°yi Ã§alÄ±ÅŸmalar dilerim! ğŸŒŸ",
                    
                    "ğŸ˜Š **Her zaman yardÄ±mcÄ± olmaktan mutluluk duyarÄ±m!**\n\nUmarÄ±m sorularÄ±nÄ±za faydalÄ± yanÄ±tlar verebildim.\n\n**ğŸ”— BaÄŸlantÄ±da KalÄ±n:**\nâ€¢ Web: www.mefapex.com\nâ€¢ LinkedIn: MEFAPEX BiliÅŸim\nâ€¢ Email: info@mefapex.com\n\n**ğŸ’¡ Ä°pucu:** Favorilerinize ekleyerek bana daha kolay ulaÅŸabilirsiniz!\n\nGÃ¶rÃ¼ÅŸÃ¼rÃ¼z! ğŸ‘‹"
                ]
            },
            
            # Yeni kategoriler
            "project_management": {
                "patterns": [
                    "proje", "project", "gÃ¶rev", "task", "deadline", "son tarih",
                    "teslim", "delivery", "milestone", "plan", "planlama"
                ],
                "responses": [
                    "ğŸ“‹ **Proje YÃ¶netimi ve SÃ¼reÃ§ler**\n\n**ğŸ¯ Proje AÅŸamalarÄ±:**\nâ€¢ **Analiz**: Ä°htiyaÃ§ belirleme ve dokÃ¼mantasyon\nâ€¢ **TasarÄ±m**: Sistem mimarisi ve UI/UX\nâ€¢ **GeliÅŸtirme**: Kodlama ve test sÃ¼reÃ§leri\nâ€¢ **Test**: QA ve kullanÄ±cÄ± testleri\nâ€¢ **Teslim**: CanlÄ±ya alma ve eÄŸitim\n\n**ğŸ“Š Proje Takibi:**\nâ€¢ HaftalÄ±k durum raporlarÄ±\nâ€¢ Sprint toplantÄ±larÄ±\nâ€¢ Milestone deÄŸerlendirmeleri\n\n**ğŸ”§ KullanÄ±lan AraÃ§lar:**\nâ€¢ Jira - GÃ¶rev yÃ¶netimi\nâ€¢ Confluence - DokÃ¼mantasyon\nâ€¢ Git - Versiyon kontrolÃ¼\nâ€¢ Slack - Ä°letiÅŸim\n\nProje durumunuz iÃ§in proje yÃ¶neticisiyle iletiÅŸime geÃ§in."
                ]
            },
            
            "training_education": {
                "patterns": [
                    "eÄŸitim", "training", "kurs", "course", "Ã¶ÄŸren", "learn",
                    "sertifika", "certificate", "rehber", "guide", "nasÄ±l"
                ],
                "responses": [
                    "ğŸ“ **EÄŸitim ve GeliÅŸim ProgramlarÄ±**\n\n**ğŸ’» Teknik EÄŸitimler:**\nâ€¢ **Programlama**: Python, JavaScript, C#\nâ€¢ **VeritabanÄ±**: SQL, MongoDB, PostgreSQL\nâ€¢ **Bulut**: AWS, Azure, Google Cloud\nâ€¢ **DevOps**: Docker, Kubernetes, CI/CD\n\n**ğŸ“š KiÅŸisel GeliÅŸim:**\nâ€¢ Proje yÃ¶netimi\nâ€¢ Ä°letiÅŸim becerileri\nâ€¢ Liderlik eÄŸitimleri\nâ€¢ Agile metodolojiler\n\n**ğŸ† Sertifikasyon DesteÄŸi:**\nâ€¢ SÄ±nav Ã¼cretleri karÅŸÄ±lanÄ±r\nâ€¢ HazÄ±rlÄ±k kurslarÄ±\nâ€¢ Mentorluk programÄ±\n\n**ğŸ“ EÄŸitim KoordinatÃ¶rÃ¼:**\nâ€¢ Email: egitim@mefapex.com\nâ€¢ Dahili: [Telefon]\n\nKariyerinizi geliÅŸtirmek iÃ§in bizimle iletiÅŸime geÃ§in!"
                ]
            }
        }
    
    def _enhance_synonyms_with_morphology(self):
        """Ä°lk soru iÅŸlendiÄŸinde eÅŸ anlamlÄ±larÄ± morfological analiz ile geniÅŸlet - ONLY when user asks a question"""
        if not hasattr(self, '_synonyms_enhanced') or not self._synonyms_enhanced:
            # Sadece detailed logging etkinse mesaj gÃ¶ster
            if self.enable_detailed_logging:
                print("ğŸ”§ Enhancing synonyms with morphological analysis...")
            
            # Ã–nce synonyms'larÄ± yÃ¼kle (eÄŸer yÃ¼klenmemiÅŸse)
            if not self._synonyms_loaded:
                # synonym_map property'sini Ã§aÄŸÄ±rarak lazy loading'i tetikle
                _ = self.synonym_map
            
            # Mevcut synonym map'i al
            current_synonyms = dict(self._synonym_map)  # Copy to avoid modification during iteration
            enhanced_synonyms = {}
            
            # Morfological analyzer'Ä± al - bu da lazy loading ile Ã§alÄ±ÅŸÄ±r
            morph_analyzer = self._get_morph_analyzer()
            
            for base_word, synonyms in current_synonyms.items():
                # Base word'Ã¼ lemmatize et
                base_lemma = morph_analyzer.lemmatize_word(base_word)
                
                # TÃ¼m eÅŸ anlamlÄ±larÄ± lemmatize et ve varyantlarÄ±nÄ± ekle
                all_variants = set()
                all_variants.add(base_word)
                all_variants.add(base_lemma)
                
                for synonym in synonyms:
                    all_variants.add(synonym)
                    synonym_lemma = morph_analyzer.lemmatize_word(synonym)
                    all_variants.add(synonym_lemma)
                    
                    # Kelime varyantlarÄ±nÄ± da ekle - sadece gerekirse
                    try:
                        variants = morph_analyzer.get_word_variants(synonym)
                        all_variants.update(variants)
                    except Exception as e:
                        # HatalarÄ± sessizce geÃ§ - verbose logging yok
                        pass
                
                # BoÅŸ stringleri filtrele
                all_variants = [v for v in all_variants if v and len(v) > 1]
                
                enhanced_synonyms[base_lemma] = list(all_variants)
                
                # Orijinal kelimenin kendisi iÃ§in de entry ekle
                if base_word != base_lemma:
                    enhanced_synonyms[base_word] = list(all_variants)
            
            # Enhanced synonyms'Ä± gÃ¼ncelle
            self._synonym_map = enhanced_synonyms
            self._synonyms_enhanced = True
            
            # Sadece detailed logging etkinse sonuÃ§ mesajÄ± gÃ¶ster
            if self.enable_detailed_logging:
                print(f"âœ… Enhanced {len(enhanced_synonyms)} synonym groups with morphological analysis")
    
    def _normalize_turkish(self, text: str) -> str:
        """TÃ¼rkÃ§e karakterleri normalize et ve morfological analysis uygula"""
        if not text:
            return text
        
        # Ã–nce tÃ¼rkÃ§e karakter dÃ¶nÃ¼ÅŸÃ¼mleri
        turkish_map = {
            'Ã§': 'c', 'ÄŸ': 'g', 'Ä±': 'i', 'Ã¶': 'o', 'ÅŸ': 's', 'Ã¼': 'u',
            'Ã‡': 'c', 'Ä': 'g', 'I': 'i', 'Ä°': 'i', 'Ã–': 'o', 'Å': 's', 'Ãœ': 'u'
        }
        
        normalized = ""
        for char in text.lower():
            normalized += turkish_map.get(char, char)
        
        return normalized
    
    def _preprocess_text(self, text: str) -> str:
        """Metni Ã¶niÅŸleme: temizleme + normalizasyon + lemmatization"""
        if not text:
            return text
        
        # Temizleme
        text = text.strip().lower()
        text = re.sub(r'[^\w\s]', ' ', text)  # Noktalama iÅŸaretlerini kaldÄ±r
        text = re.sub(r'\s+', ' ', text)  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        
        # Lemmatization (lazy loading ile)
        morph_analyzer = self._get_morph_analyzer()
        lemmatized_text = morph_analyzer.lemmatize_text(text)
        
        # TÃ¼rkÃ§e karakter normalizasyonu (son adÄ±m olarak)
        normalized_text = self._normalize_turkish(lemmatized_text)
        
        return normalized_text
    
    def _expand_with_synonyms(self, text: str) -> List[str]:
        """Metni eÅŸ anlamlÄ± kelimelerle ve morfological varyantlarla geniÅŸlet"""
        # Ä°lk kullanÄ±mda morfological geniÅŸletmeyi yap
        self._enhance_synonyms_with_morphology()
        
        words = text.split()
        expanded_texts = {text}  # Set kullanarak dublicate'leri Ã¶nle
        
        # Lazy loading ile morphological analyzer'Ä± al
        morph_analyzer = self._get_morph_analyzer()
        
        # Her kelime iÃ§in eÅŸ anlamlÄ±larÄ± ve varyantlarÄ±nÄ± kontrol et
        for word in words:
            # Kelimeyi lemmatize et
            lemma = morph_analyzer.lemmatize_word(word)
            
            # Normalized versiyonlarÄ± da kontrol et
            normalized_word = self._normalize_turkish(word)
            normalized_lemma = self._normalize_turkish(lemma)
            
            # EÅŸ anlamlÄ±larÄ± bul
            synonyms_found = set()
            
            # Orijinal kelime iÃ§in eÅŸ anlamlÄ±larÄ±
            for key in [word, lemma, normalized_word, normalized_lemma]:
                if key in self.synonym_map:
                    synonyms_found.update(self.synonym_map[key])
            
            # Morfological varyantlarÄ± da ekle
            variants = morph_analyzer.get_word_variants(word)
            for variant in variants:
                normalized_variant = self._normalize_turkish(variant)
                for key in [variant, normalized_variant]:
                    if key in self.synonym_map:
                        synonyms_found.update(self.synonym_map[key])
            
            # Bulunan eÅŸ anlamlÄ±larÄ± ile yeni metinler oluÅŸtur
            for synonym in synonyms_found:
                if synonym and synonym != word:
                    # Orijinal kelimeyi eÅŸ anlamlÄ±sÄ±yla deÄŸiÅŸtir
                    new_text = text.replace(word, synonym)
                    expanded_texts.add(new_text)
                    
                    # Lemmatized versiyonlarÄ±nÄ± da dene
                    lemma_text = text.replace(word, morph_analyzer.lemmatize_word(synonym))
                    expanded_texts.add(lemma_text)
        
        return list(expanded_texts)
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Ä°ki metin arasÄ±nda morfological analysis destekli benzerlik hesapla"""
        # Cache kontrolÃ¼
        cache_key = f"{text1}|{text2}"
        if cache_key in self.pattern_cache:
            return self.pattern_cache[cache_key]
        
        # Metinleri Ã¶niÅŸle
        processed1 = self._preprocess_text(text1)
        processed2 = self._preprocess_text(text2)
        
        # Temel benzerlik
        basic_similarity = SequenceMatcher(None, processed1, processed2).ratio()
        
        # Kelime bazlÄ± benzerlik (lemmatized)
        words1 = set(processed1.split())
        words2 = set(processed2.split())
        
        if not words1 or not words2:
            similarity = basic_similarity
        else:
            # Exact match
            word_similarity = len(words1 & words2) / len(words1 | words2)
            
            # Semantic similarity (synonym-based)
            semantic_matches = 0
            total_comparisons = 0
            
            for w1 in words1:
                for w2 in words2:
                    total_comparisons += 1
                    if w1 == w2:
                        semantic_matches += 1
                    else:
                        # Check if words are synonyms
                        w1_synonyms = set()
                        w2_synonyms = set()
                        
                        # Collect synonyms for w1
                        for key, synonyms in self.synonym_map.items():
                            if w1 in synonyms or w1 == key:
                                w1_synonyms.update(synonyms)
                                w1_synonyms.add(key)
                        
                        # Collect synonyms for w2
                        for key, synonyms in self.synonym_map.items():
                            if w2 in synonyms or w2 == key:
                                w2_synonyms.update(synonyms)
                                w2_synonyms.add(key)
                        
                        # Check if there's overlap
                        if w1_synonyms & w2_synonyms:
                            semantic_matches += 0.8  # Synonym match weight
            
            if total_comparisons > 0:
                semantic_similarity = semantic_matches / total_comparisons
            else:
                semantic_similarity = 0
            
            # AÄŸÄ±rlÄ±klÄ± ortalama
            similarity = (basic_similarity * 0.4) + (word_similarity * 0.4) + (semantic_similarity * 0.2)
        
        # Cache'e kaydet
        self.pattern_cache[cache_key] = similarity
        return similarity
    
    def find_best_match(self, user_input: str, threshold: float = 0.3) -> Optional[Dict]:
        """KullanÄ±cÄ± girdisi iÃ§in en iyi eÅŸleÅŸmeyi bul (morfological analysis ile)"""
        if not user_input or not user_input.strip():
            return None
        
        # KullanÄ±cÄ± girdisini Ã¶niÅŸle
        processed_input = self._preprocess_text(user_input)
        
        best_match = None
        best_score = 0.0
        
        # EÅŸ anlamlÄ± kelimelerle ve morfological varyantlarla geniÅŸletilmiÅŸ versiyonlarÄ± da kontrol et
        expanded_inputs = self._expand_with_synonyms(processed_input)
        
        # Orijinal girdiyi de ekle
        all_inputs = [user_input.strip().lower(), processed_input] + expanded_inputs
        all_inputs = list(set(all_inputs))  # Duplicate'leri kaldÄ±r
        
        for category, data in self.responses.items():
            patterns = data["patterns"]
            responses = data["responses"]
            
            for pattern in patterns:
                # Pattern'i de Ã¶niÅŸle
                processed_pattern = self._preprocess_text(pattern)
                
                # TÃ¼m girdi versiyonlarÄ±nÄ± kontrol et
                for input_variant in all_inputs:
                    # Direkt iÃ§erme kontrolÃ¼ (yÃ¼ksek puan)
                    if processed_pattern in input_variant or input_variant in processed_pattern:
                        score = 0.95
                    elif pattern.lower() in user_input.lower() or user_input.lower() in pattern.lower():
                        score = 0.9  # Orijinal metin eÅŸleÅŸmesi
                    else:
                        # Morfological similarity
                        score = self._calculate_similarity(input_variant, processed_pattern)
                    
                    if score > best_score and score >= threshold:
                        best_score = score
                        best_match = {
                            "category": category,
                            "pattern": pattern,
                            "responses": responses,
                            "score": score,
                            "matched_input": input_variant,
                            "processed_pattern": processed_pattern
                        }
        
        # EÄŸer hala eÅŸleÅŸme yoksa, daha esnek arama yap
        if not best_match or best_score < 0.5:
            # Lemmatized kelime bazlÄ± arama
            input_lemmas = set(self._preprocess_text(user_input).split())
            
            for category, data in self.responses.items():
                patterns = data["patterns"]
                responses = data["responses"]
                
                for pattern in patterns:
                    pattern_lemmas = set(self._preprocess_text(pattern).split())
                    
                    if input_lemmas and pattern_lemmas:
                        # Lemma intersection score
                        intersection = len(input_lemmas & pattern_lemmas)
                        union = len(input_lemmas | pattern_lemmas)
                        
                        if union > 0:
                            lemma_score = intersection / union
                            
                            # Bonus for multiple word matches
                            if intersection > 1:
                                lemma_score *= 1.2
                            
                            if lemma_score > best_score and lemma_score >= (threshold * 0.8):
                                best_score = lemma_score
                                best_match = {
                                    "category": category,
                                    "pattern": pattern,
                                    "responses": responses,
                                    "score": lemma_score,
                                    "matched_input": user_input,
                                    "processed_pattern": pattern,
                                    "match_type": "lemma_based"
                                }
        
        return best_match
    
    def get_response(self, user_input: str) -> str:
        """KullanÄ±cÄ± girdisi iÃ§in yanÄ±t Ã¼ret"""
        # Ã–nce static responses'dan ara
        match = self.find_best_match(user_input)
        
        if match:
            responses = match["responses"]
            import random
            response = random.choice(responses)
            # Sadece detailed logging etkinse debug mesajÄ± gÃ¶ster
            if self.enable_detailed_logging:
                print(f"ğŸ¯ Static response matched: {match['category']} (score: {match['score']:.2f})")
            return response
        
        # Fallback yanÄ±tlarÄ±
        return self._get_fallback_response(user_input)
    
    def _get_fallback_response(self, user_input: str) -> str:
        """EÅŸleÅŸme bulunamadÄ±ÄŸÄ±nda fallback yanÄ±tlarÄ±"""
        fallback_responses = [
            f"ğŸ¤” **\"{user_input}\" konusunda size yardÄ±mcÄ± olmaya Ã§alÄ±ÅŸÄ±yorum...**\n\n"
            "Bu konuda elimde yeterli bilgi bulunmuyor, ancak size ÅŸu ÅŸekillerde yardÄ±mcÄ± olabilirim:\n\n"
            "â€¢ ğŸ“ **Teknik Destek**: destek@mefapex.com\n"
            "â€¢ ğŸ’¬ **Genel Sorular**: info@mefapex.com\n"
            "â€¢ ğŸ†˜ **Acil Durumlar**: [Acil telefon]\n\n"
            "LÃ¼tfen sorunuzu daha detaylandÄ±rÄ±r mÄ±sÄ±nÄ±z? BÃ¶ylece size daha iyi yardÄ±mcÄ± olabilirim.",
            
            f"ğŸ’¡ **\"{user_input}\" hakkÄ±nda bilgi arÄ±yorsunuz...**\n\n"
            "Bu konu iÃ§in size en doÄŸru bilgiyi ÅŸu kanallardan alabilirsiniz:\n\n"
            "â€¢ ğŸ¢ **Ä°lgili Departman**: Konuyla ilgili uzman ekibimiz\n"
            "â€¢ ğŸ“‹ **DokÃ¼mantasyon**: Åirket iÃ§i rehberler\n"
            "â€¢ ğŸ‘¥ **MeslektaÅŸlarÄ±nÄ±z**: Deneyimli ekip Ã¼yeleri\n\n"
            "Ben de Ã¶ÄŸrenmeye devam ediyorum. Sorunuzu biraz daha aÃ§abilir misiniz?",
            
            f"ğŸ” **\"{user_input}\" konusunu araÅŸtÄ±rÄ±yorum...**\n\n"
            "Åu anda bu konuda size kapsamlÄ± bir yanÄ±t veremiyorum, ama:\n\n"
            "â€¢ ğŸ“ **HÄ±zlÄ± Ã‡Ã¶zÃ¼m**: Ä°lgili departmanÄ± arayÄ±n\n"
            "â€¢ ğŸ’» **Online Kaynaklar**: Åirket portalÄ±nÄ± kontrol edin\n"
            "â€¢ ğŸ“ **Ticket AÃ§Ä±n**: Sistemden destek talebi oluÅŸturun\n\n"
            "Bu arada bana daha spesifik sorular sorabilirsiniz. Ã–rneÄŸin teknik, Ä°K, gÃ¼venlik konularÄ±nda size yardÄ±mcÄ± olabilirim!"
        ]
        
        import random
        return random.choice(fallback_responses)
    
    def get_category_suggestions(self, user_input: str) -> List[str]:
        """KullanÄ±cÄ± girdisine gÃ¶re kategori Ã¶nerileri"""
        suggestions = []
        user_input_lower = user_input.lower()
        
        # Anahtar kelimeler bazÄ±nda Ã¶neriler
        if any(word in user_input_lower for word in ['saat', 'Ã§alÄ±ÅŸma', 'mesai', 'aÃ§Ä±k', 'kapalÄ±']):
            suggestions.append("Ã‡alÄ±ÅŸma saatleri hakkÄ±nda bilgi almak iÃ§in: 'Ã§alÄ±ÅŸma saatleri' yazÄ±n")
        
        if any(word in user_input_lower for word in ['destek', 'help', 'problem', 'hata']):
            suggestions.append("Teknik destek iÃ§in: 'teknik destek' yazÄ±n")
        
        if any(word in user_input_lower for word in ['izin', 'tatil', 'ik', 'personel']):
            suggestions.append("Ä°nsan kaynaklarÄ± iÃ§in: 'izin baÅŸvurusu' yazÄ±n")
        
        if any(word in user_input_lower for word in ['gÃ¼venlik', 'kural', 'kaza']):
            suggestions.append("GÃ¼venlik bilgileri iÃ§in: 'gÃ¼venlik kurallarÄ±' yazÄ±n")
        
        if any(word in user_input_lower for word in ['ÅŸirket', 'mefapex', 'hakkÄ±nda']):
            suggestions.append("Åirket bilgileri iÃ§in: 'MEFAPEX hakkÄ±nda' yazÄ±n")
        
        return suggestions[:3]  # En fazla 3 Ã¶neri
    
    def get_stats(self) -> Dict:
        """Content manager istatistikleri"""
        total_patterns = sum(len(data["patterns"]) for data in self.responses.values())
        total_responses = sum(len(data["responses"]) for data in self.responses.values())
        
        # Morfological analyzer stats (lazy loading aware)
        if self.morph_analyzer is None:
            morph_stats = {
                "morphological_analyzer": "Not loaded (lazy loading - will initialize on first use)",
                "fallback_lemmas": "Available",
                "morphological_analysis": "On-demand"
            }
        else:
            morph_stats = {
                "spacy_available": self.morph_analyzer.nlp is not None,
                "fallback_lemmas": len(self.morph_analyzer.fallback_lemmas),
                "morphological_analysis": "Loaded and active"
            }
        
        # Synonym stats (lazy loading aware)
        if not self._synonyms_loaded:
            synonym_stats = {
                "synonyms": "Not loaded (lazy loading - will load on first use)",
                "enhanced_with_morphology": "Pending"
            }
        else:
            synonym_stats = {
                "synonym_words": len(self.synonym_map),
                "enhanced_with_morphology": self._synonyms_enhanced
            }
        
        return {
            "total_categories": len(self.responses),
            "total_patterns": total_patterns,
            "total_responses": total_responses,
            "cache_size": len(self.pattern_cache),
            "language": "Turkish (Enhanced with On-Demand Morphological Analysis)",
            "morphological_analysis": morph_stats,
            "synonym_analysis": synonym_stats
        }

# Global instance - ultra efficient mode (no detailed logging, ERROR level only, completely silent)
# Morfological analysis sadece kullanÄ±cÄ± soru sorduÄŸunda Ã§alÄ±ÅŸÄ±r
improved_turkish_content = ImprovedTurkishContentManager(enable_detailed_logging=False)
