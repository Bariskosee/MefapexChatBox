"""
ðŸ‡¹ðŸ‡· GeliÅŸmiÅŸ TÃ¼rkÃ§e Content Manager
==================================
Daha iyi TÃ¼rkÃ§e yanÄ±tlar iÃ§in optimize edilmiÅŸ content yÃ¶neticisi
Morfological analysis ve lemmatization ile desteklenmiÅŸ
"""
import json
import re
import logging
import os
from typing import Dict, List, Tuple, Optional, Set
from difflib import SequenceMatcher

# Turkish NLP dependencies
try:
    import spacy
    from spacy.cli import download
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    spacy = None

logger = logging.getLogger(__name__)

class TurkishMorphAnalyzer:
    """
    TÃ¼rkÃ§e morfological analysis ve lemmatization sÄ±nÄ±fÄ±
    spaCy Turkish model kullanÄ±r
    """
    
    def __init__(self):
        self.nlp = None
        self.fallback_lemmas = self._load_fallback_lemmas()
        self._initialize_spacy()
    
    def _initialize_spacy(self):
        """spaCy Turkish modelini baÅŸlat"""
        if not SPACY_AVAILABLE:
            logger.warning("ðŸš« spaCy not available. Using fallback morphological analysis.")
            return
        
        try:
            # Try to load Turkish model
            try:
                self.nlp = spacy.load("tr_core_news_sm")
                logger.info("âœ… Turkish spaCy model loaded successfully")
            except OSError:
                # Model not found, log warning but don't try to download
                logger.warning("âš ï¸ Turkish spaCy model (tr_core_news_sm) not found.")
                logger.info("ðŸ”„ Using fallback morphological analysis")
                logger.info("ðŸ’¡ To install Turkish model: python -m spacy download tr_core_news_sm")
                self.nlp = None
        except Exception as e:
            logger.error(f"âŒ Error initializing spaCy: {e}")
            self.nlp = None
    
    def _load_fallback_lemmas(self) -> Dict[str, str]:
        """Fallback lemma sÃ¶zlÃ¼ÄŸÃ¼ yÃ¼kle"""
        return {
            # Fiiller (Verbs)
            'Ã§alÄ±ÅŸÄ±yor': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸÄ±yorum': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸÄ±yorsun': 'Ã§alÄ±ÅŸ',
            'Ã§alÄ±ÅŸan': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸmak': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸtÄ±': 'Ã§alÄ±ÅŸ',
            'gidiyor': 'git', 'gidiyorum': 'git', 'gitti': 'git', 'gitmek': 'git',
            'geliyor': 'gel', 'geliyorum': 'gel', 'geldi': 'gel', 'gelmek': 'gel',
            'yapÄ±yor': 'yap', 'yapÄ±yorum': 'yap', 'yaptÄ±': 'yap', 'yapmak': 'yap',
            'oluyor': 'ol', 'oluyorum': 'ol', 'oldu': 'ol', 'olmak': 'ol',
            'istiyor': 'iste', 'istiyorum': 'iste', 'istedi': 'iste', 'istemek': 'iste',
            'baÅŸlÄ±yor': 'baÅŸla', 'baÅŸlÄ±yorum': 'baÅŸla', 'baÅŸladÄ±': 'baÅŸla', 'baÅŸlamak': 'baÅŸla',
            'bitiyor': 'bit', 'bitiyorum': 'bit', 'bitti': 'bit', 'bitmek': 'bit',
            'aÃ§Ä±yor': 'aÃ§', 'aÃ§Ä±yorum': 'aÃ§', 'aÃ§tÄ±': 'aÃ§', 'aÃ§mak': 'aÃ§',
            'kapÄ±yor': 'kapa', 'kapÄ±yorum': 'kapa', 'kapattÄ±': 'kapa', 'kapamak': 'kapa',
            'alÄ±yor': 'al', 'alÄ±yorum': 'al', 'aldÄ±': 'al', 'almak': 'al',
            'veriyor': 'ver', 'veriyorum': 'ver', 'verdi': 'ver', 'vermek': 'ver',
            'buluyor': 'bul', 'buluyorum': 'bul', 'buldu': 'bul', 'bulmak': 'bul',
            'dÃ¼ÅŸÃ¼nÃ¼yor': 'dÃ¼ÅŸÃ¼n', 'dÃ¼ÅŸÃ¼nÃ¼yorum': 'dÃ¼ÅŸÃ¼n', 'dÃ¼ÅŸÃ¼ndÃ¼': 'dÃ¼ÅŸÃ¼n',
            'konuÅŸuyor': 'konuÅŸ', 'konuÅŸuyorum': 'konuÅŸ', 'konuÅŸtu': 'konuÅŸ',
            'Ã§alÄ±ÅŸmasÄ±': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸmasÄ±nÄ±': 'Ã§alÄ±ÅŸ', 'Ã§alÄ±ÅŸmasÄ±na': 'Ã§alÄ±ÅŸ',
            
            # Ä°simler (Nouns) - Ã‡oÄŸul ve hal ekleri
            'saatleri': 'saat', 'saatler': 'saat', 'saatin': 'saat', 'saate': 'saat',
            'gÃ¼nleri': 'gÃ¼n', 'gÃ¼nler': 'gÃ¼n', 'gÃ¼nÃ¼n': 'gÃ¼n', 'gÃ¼ne': 'gÃ¼n',
            'iÅŸleri': 'iÅŸ', 'iÅŸler': 'iÅŸ', 'iÅŸin': 'iÅŸ', 'iÅŸe': 'iÅŸ',
            'sorunlarÄ±': 'sorun', 'sorunlar': 'sorun', 'sorunun': 'sorun', 'soruna': 'sorun',
            'hatalarÄ±': 'hata', 'hatalar': 'hata', 'hatanÄ±n': 'hata', 'hataya': 'hata',
            'sistemleri': 'sistem', 'sistemler': 'sistem', 'sistemin': 'sistem', 'sisteme': 'sistem',
            'bilgileri': 'bilgi', 'bilgiler': 'bilgi', 'bilginin': 'bilgi', 'bilgiye': 'bilgi',
            'desteÄŸi': 'destek', 'destekler': 'destek', 'desteÄŸin': 'destek', 'desteÄŸe': 'destek',
            'gÃ¼venliÄŸi': 'gÃ¼venlik', 'gÃ¼venlikler': 'gÃ¼venlik', 'gÃ¼venliÄŸin': 'gÃ¼venlik',
            'personeli': 'personel', 'personeller': 'personel', 'personelin': 'personel',
            'projesi': 'proje', 'projeler': 'proje', 'projenin': 'proje', 'projeye': 'proje',
            'eÄŸitimi': 'eÄŸitim', 'eÄŸitimler': 'eÄŸitim', 'eÄŸitimin': 'eÄŸitim', 'eÄŸitime': 'eÄŸitim',
            'kullanÄ±cÄ±sÄ±': 'kullanÄ±cÄ±', 'kullanÄ±cÄ±lar': 'kullanÄ±cÄ±', 'kullanÄ±cÄ±nÄ±n': 'kullanÄ±cÄ±',
            
            # SÄ±fatlar (Adjectives)
            'iyi': 'iyi', 'iyiler': 'iyi', 'iyidir': 'iyi', 'iyiyi': 'iyi',
            'kÃ¶tÃ¼': 'kÃ¶tÃ¼', 'kÃ¶tÃ¼ler': 'kÃ¶tÃ¼', 'kÃ¶tÃ¼dÃ¼r': 'kÃ¶tÃ¼', 'kÃ¶tÃ¼yÃ¼': 'kÃ¶tÃ¼',
            'hÄ±zlÄ±': 'hÄ±zlÄ±', 'hÄ±zlÄ±lar': 'hÄ±zlÄ±', 'hÄ±zlÄ±dÄ±r': 'hÄ±zlÄ±',
            'yavaÅŸ': 'yavaÅŸ', 'yavaÅŸlar': 'yavaÅŸ', 'yavaÅŸtÄ±r': 'yavaÅŸ',
            'kolay': 'kolay', 'kolaylar': 'kolay', 'kolaydÄ±r': 'kolay',
            'zor': 'zor', 'zorlar': 'zor', 'zordur': 'zor',
            'yeni': 'yeni', 'yeniler': 'yeni', 'yenidir': 'yeni',
            'eski': 'eski', 'eskiler': 'eski', 'eskidir': 'eski',
            'bÃ¼yÃ¼k': 'bÃ¼yÃ¼k', 'bÃ¼yÃ¼kler': 'bÃ¼yÃ¼k', 'bÃ¼yÃ¼ktÃ¼r': 'bÃ¼yÃ¼k',
            'kÃ¼Ã§Ã¼k': 'kÃ¼Ã§Ã¼k', 'kÃ¼Ã§Ã¼kler': 'kÃ¼Ã§Ã¼k', 'kÃ¼Ã§Ã¼ktÃ¼r': 'kÃ¼Ã§Ã¼k',
            
            # Zamanlar (Time expressions)
            'bugÃ¼n': 'bugÃ¼n', 'bugÃ¼nÃ¼': 'bugÃ¼n', 'bugÃ¼ne': 'bugÃ¼n',
            'yarÄ±n': 'yarÄ±n', 'yarÄ±nÄ±': 'yarÄ±n', 'yarÄ±na': 'yarÄ±n',
            'dÃ¼n': 'dÃ¼n', 'dÃ¼nÃ¼': 'dÃ¼n', 'dÃ¼ne': 'dÃ¼n',
            'ÅŸimdi': 'ÅŸimdi', 'ÅŸimdiyi': 'ÅŸimdi', 'ÅŸimdiye': 'ÅŸimdi',
            'sonra': 'sonra', 'sonrasÄ±': 'sonra', 'sonrasÄ±na': 'sonra',
            'Ã¶nce': 'Ã¶nce', 'Ã¶ncesi': 'Ã¶nce', 'Ã¶ncesine': 'Ã¶nce',
        }
    
    def lemmatize_word(self, word: str) -> str:
        """Tek kelimeyi lemmatize et"""
        if not word:
            return word
        
        word_lower = word.lower()
        
        # spaCy kullan (varsa)
        if self.nlp:
            try:
                doc = self.nlp(word_lower)
                if doc and len(doc) > 0:
                    lemma = doc[0].lemma_
                    if lemma and lemma != word_lower:
                        return lemma
            except Exception as e:
                logger.debug(f"spaCy lemmatization error for '{word}': {e}")
        
        # Fallback lemma sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ kullan
        if word_lower in self.fallback_lemmas:
            return self.fallback_lemmas[word_lower]
        
        # Temel morfological rules
        return self._apply_basic_rules(word_lower)
    
    def _apply_basic_rules(self, word: str) -> str:
        """Temel morfological kurallarÄ± uygula"""
        if len(word) < 3:
            return word
        
        # Ã‡oÄŸul ekleri (-ler, -lar)
        if word.endswith(('ler', 'lar')):
            base = word[:-3]
            if len(base) >= 2:
                return base
        
        # Ä°yelik ekleri (-i, -Ä±, -u, -Ã¼)
        if word.endswith(('larÄ±', 'leri', 'larÄ±', 'leri')):
            base = word[:-4]
            if len(base) >= 2:
                return base
        
        # Hal ekleri
        if word.endswith(('nÄ±n', 'nin', 'nun', 'nÃ¼n')):
            base = word[:-3]
            if len(base) >= 2:
                return base
        
        if word.endswith(('na', 'ne', 'ya', 'ye')):
            base = word[:-2]
            if len(base) >= 2:
                return base
        
        # Fiil ekleri
        if word.endswith(('Ä±yor', 'iyor', 'uyor', 'Ã¼yor')):
            base = word[:-4]
            if len(base) >= 2:
                return base
        
        if word.endswith(('mak', 'mek')):
            base = word[:-3]
            if len(base) >= 2:
                return base
        
        return word
    
    def lemmatize_text(self, text: str) -> str:
        """Metindeki tÃ¼m kelimeleri lemmatize et"""
        if not text:
            return text
        
        words = text.split()
        lemmatized_words = []
        
        for word in words:
            # Noktalama iÅŸaretlerini ayÄ±r
            clean_word = re.sub(r'[^\w\s]', '', word)
            if clean_word:
                lemma = self.lemmatize_word(clean_word)
                lemmatized_words.append(lemma)
        
        return ' '.join(lemmatized_words)
    
    def get_word_variants(self, word: str) -> Set[str]:
        """Bir kelimenin farklÄ± varyantlarÄ±nÄ± Ã¼ret"""
        variants = {word.lower()}
        lemma = self.lemmatize_word(word)
        variants.add(lemma)
        
        # Temel varyantlar ekle
        if lemma in self.fallback_lemmas.values():
            # Bu lemma iÃ§in bilinen tÃ¼m formlarÄ± bul
            for inflected, base in self.fallback_lemmas.items():
                if base == lemma:
                    variants.add(inflected)
        
        return variants

logger = logging.getLogger(__name__)

class ImprovedTurkishContentManager:
    """
    GeliÅŸmiÅŸ TÃ¼rkÃ§e content yÃ¶neticisi
    - Daha kapsamlÄ± static responses
    - Morfological analysis ile geliÅŸmiÅŸ matching
    - TÃ¼rkÃ§e karakter desteÄŸi
    - Lemmatization ile akÄ±llÄ± eÅŸleÅŸtirme
    - AkÄ±llÄ± fallback responses
    """
    
    def __init__(self):
        self.morph_analyzer = TurkishMorphAnalyzer()
        self.responses = self._load_enhanced_responses()
        self.synonym_map = self._load_synonyms_from_file()
        self.pattern_cache = {}
        
        logger.info("ðŸ‡¹ðŸ‡· Enhanced Turkish Content Manager with morphological analysis initialized")
    
    def _load_synonyms_from_file(self) -> Dict[str, List[str]]:
        """synonyms.json dosyasÄ±ndan eÅŸ anlamlÄ± kelimeleri yÃ¼kle"""
        try:
            # Ã–nce mevcut dizinde ara
            synonyms_path = "content/synonyms.json"
            if not os.path.exists(synonyms_path):
                # Alternatif yollarÄ± dene
                possible_paths = [
                    "./content/synonyms.json",
                    "../content/synonyms.json",
                    "/Users/bariskose/Downloads/MefapexChatBox-main/content/synonyms.json"
                ]
                for path in possible_paths:
                    if os.path.exists(path):
                        synonyms_path = path
                        break
            
            with open(synonyms_path, 'r', encoding='utf-8') as f:
                loaded_synonyms = json.load(f)
            
            # Lemmatization ile eÅŸ anlamlÄ±larÄ± geniÅŸlet
            enhanced_synonyms = {}
            for base_word, synonyms in loaded_synonyms.items():
                # Base word'Ã¼ lemmatize et
                base_lemma = self.morph_analyzer.lemmatize_word(base_word)
                
                # TÃ¼m eÅŸ anlamlÄ±larÄ± lemmatize et ve varyantlarÄ±nÄ± ekle
                all_variants = set()
                all_variants.add(base_word)
                all_variants.add(base_lemma)
                
                for synonym in synonyms:
                    all_variants.add(synonym)
                    synonym_lemma = self.morph_analyzer.lemmatize_word(synonym)
                    all_variants.add(synonym_lemma)
                    
                    # Kelime varyantlarÄ±nÄ± da ekle
                    variants = self.morph_analyzer.get_word_variants(synonym)
                    all_variants.update(variants)
                
                # BoÅŸ stringleri filtrele
                all_variants = [v for v in all_variants if v and len(v) > 1]
                
                enhanced_synonyms[base_lemma] = list(all_variants)
                
                # Orijinal kelimenin kendisi iÃ§in de entry ekle
                if base_word != base_lemma:
                    enhanced_synonyms[base_word] = list(all_variants)
            
            logger.info(f"ðŸ“š Loaded and enhanced {len(enhanced_synonyms)} synonym groups from file")
            return enhanced_synonyms
            
        except Exception as e:
            logger.warning(f"âš ï¸ Could not load synonyms from file: {e}")
            return self._create_synonym_map()  # Fallback to hardcoded synonyms
    
    def _create_synonym_map(self) -> Dict[str, List[str]]:
        """Fallback: TÃ¼rkÃ§e eÅŸ anlamlÄ± kelimeler (hard-coded)"""
        return {
            'Ã§alÄ±ÅŸ': ['iÅŸ', 'mesai', 'gÃ¶rev', 'vazife', 'work', 'job', 'Ã§alÄ±ÅŸma'],
            'saat': ['zaman', 'vakit', 'time', 'hours', 'hour'],
            'aÃ§': ['open', 'baÅŸla', 'start', 'begin', 'aÃ§Ä±k'],
            'kapa': ['closed', 'bit', 'end', 'finish', 'stop', 'kapalÄ±'],
            'destek': ['yardÄ±m', 'help', 'support', 'assistance', 'aid'],
            'sorun': ['problem', 'hata', 'error', 'issue', 'bug'],
            'mefapex': ['ÅŸirket', 'company', 'firma', 'organization', 'kurum'],
            'gÃ¼venlik': ['security', 'safety', 'emniyet', 'koruma'],
            'izin': ['leave', 'vacation', 'tatil', 'permit', 'permission'],
            'proje': ['project', 'gÃ¶rev', 'task', 'iÅŸ', 'work'],
            'eÄŸitim': ['training', 'education', 'kurs', 'course', 'Ã¶ÄŸretim'],
            'sistem': ['system', 'software', 'yazÄ±lÄ±m', 'program'],
            'hata': ['error', 'bug', 'problem', 'sorun', 'issue'],
            'yardÄ±m': ['help', 'support', 'destek', 'assistance']
        }
    
    def _load_enhanced_responses(self) -> Dict:
        """GeliÅŸmiÅŸ TÃ¼rkÃ§e yanÄ±tlar yÃ¼kle"""
        return {
            "greeting": {
                "patterns": [
                    "merhaba", "selam", "selamlar", "selamun aleykÃ¼m", "gÃ¼naydÄ±n", 
                    "iyi gÃ¼nler", "iyi akÅŸamlar", "iyi geceler", "nasÄ±lsÄ±n", 
                    "nasÄ±l gidiyor", "naber", "hello", "hi", "hey"
                ],
                "responses": [
                    "ðŸ™‹â€â™‚ï¸ **Merhaba! HoÅŸ geldiniz MEFAPEX'e!**\n\nBen sizin AI asistanÄ±nÄ±zÄ±m. Size ÅŸu konularda yardÄ±mcÄ± olabilirim:\n\nâ€¢ ðŸ­ **Fabrika OperasyonlarÄ±**: Ã‡alÄ±ÅŸma saatleri, vardiya bilgileri\nâ€¢ ðŸ’» **Teknik Destek**: YazÄ±lÄ±m, sistem ve IT sorunlarÄ±\nâ€¢ ðŸ‘¥ **Ä°nsan KaynaklarÄ±**: Ä°zin baÅŸvurularÄ±, personel iÅŸlemleri\nâ€¢ ðŸ›¡ï¸ **GÃ¼venlik**: Ä°ÅŸ gÃ¼venliÄŸi kurallarÄ± ve prosedÃ¼rler\nâ€¢ ðŸ¢ **Åžirket Bilgileri**: MEFAPEX hizmetleri ve politikalarÄ±\n\nSorularÄ±nÄ±zÄ± bekliyorum! ðŸ˜Š",
                    
                    "ðŸ‘‹ **HoÅŸ geldiniz!**\n\nMEFAPEX BiliÅŸim Teknolojileri'nin AI asistanÄ±yÄ±m. Size nasÄ±l yardÄ±mcÄ± olabilirim?\n\n**PopÃ¼ler Sorular:**\nâ€¢ \"Ã‡alÄ±ÅŸma saatleri nelerdir?\"\nâ€¢ \"Teknik destek nasÄ±l alabilirim?\"\nâ€¢ \"Ä°zin baÅŸvurusu nasÄ±l yapÄ±lÄ±r?\"\nâ€¢ \"GÃ¼venlik kurallarÄ± nelerdir?\"\n\nDetaylÄ± bilgi iÃ§in sorunuzu yazabilirsiniz! ðŸš€"
                ]
            },
            
            "working_hours": {
                "patterns": [
                    "Ã§alÄ±ÅŸma saat", "iÅŸ saat", "mesai", "vardiya", "kaÃ§ta aÃ§Ä±k", 
                    "kaÃ§ta kapalÄ±", "ne zaman aÃ§Ä±k", "working hours", "office hours",
                    "Ã§alÄ±ÅŸma zamanÄ±", "iÅŸ zamanÄ±", "aÃ§Ä±lÄ±ÅŸ", "kapanÄ±ÅŸ", "ofis saat"
                ],
                "responses": [
                    "â° **MEFAPEX Ã‡alÄ±ÅŸma Saatleri**\n\n**ðŸ“… Hafta Ä°Ã§i Ã‡alÄ±ÅŸma:**\nâ€¢ Pazartesi - Cuma: 09:00 - 18:00\nâ€¢ Ã–ÄŸle molasÄ±: 12:00 - 13:00\n\n**ðŸ“… Hafta Sonu:**\nâ€¢ Cumartesi: 09:00 - 13:00 (YarÄ±m gÃ¼n)\nâ€¢ Pazar: KapalÄ±\n\n**ðŸš¨ Acil Durumlar:**\nâ€¢ 7/24 teknik destek hattÄ±\nâ€¢ Kritik sistem arÄ±zalarÄ± iÃ§in anÄ±nda mÃ¼dahale\n\n**ðŸ“ž Ä°letiÅŸim:**\nâ€¢ Ana hat: [Telefon numarasÄ±]\nâ€¢ Acil: [Acil telefon]\nâ€¢ Email: info@mefapex.com",
                    
                    "ðŸ•˜ **Ä°ÅŸ Saatleri ve Vardiya Bilgileri**\n\n**Standart Mesai:**\nâ€¢ BaÅŸlangÄ±Ã§: 09:00\nâ€¢ BitiÅŸ: 18:00\nâ€¢ Toplam: 8 saat + 1 saat mola\n\n**Mola Saatleri:**\nâ€¢ Ã–ÄŸle arasÄ±: 12:00-13:00\nâ€¢ Ã‡ay molasÄ±: 10:00 ve 15:30\n\n**Esnek Ã‡alÄ±ÅŸma:**\nâ€¢ Uzaktan Ã§alÄ±ÅŸma imkanÄ±\nâ€¢ Esnek baÅŸlangÄ±Ã§ saatleri (08:00-10:00)\n\nDetaylÄ± bilgi iÃ§in Ä°K departmanÄ±yla iletiÅŸime geÃ§in."
                ]
            },
            
            "technical_support": {
                "patterns": [
                    "teknik destek", "technical support", "yardÄ±m", "help", "problem", 
                    "sorun", "hata", "error", "arÄ±za", "breakdown", "sistem", "yazÄ±lÄ±m",
                    "bilgisayar", "network", "aÄŸ", "server", "sunucu"
                ],
                "responses": [
                    "ðŸ› ï¸ **MEFAPEX Teknik Destek**\n\n**ðŸ“ž AnÄ±nda Destek:**\nâ€¢ Teknik Destek HattÄ±: [Telefon]\nâ€¢ Email: destek@mefapex.com\nâ€¢ CanlÄ± Chat: Bu platform Ã¼zerinden\n\n**ðŸ”§ Destek TÃ¼rleri:**\nâ€¢ **YazÄ±lÄ±m DesteÄŸi**: Uygulama hatalarÄ±, gÃ¼ncelleme\nâ€¢ **Sistem DesteÄŸi**: Sunucu, aÄŸ altyapÄ±sÄ±\nâ€¢ **Acil MÃ¼dahale**: Kritik sistem Ã§Ã¶kmeleri\nâ€¢ **KullanÄ±cÄ± EÄŸitimi**: Sistem kullanÄ±m rehberi\n\n**â±ï¸ YanÄ±t SÃ¼releri:**\nâ€¢ Acil: 15 dakika\nâ€¢ Normal: 2 saat\nâ€¢ PlanlÄ±: 24 saat\n\nProbleminizi detaylandÄ±rÄ±n, hemen yardÄ±mcÄ± olalÄ±m! ðŸš€",
                    
                    "ðŸ’» **IT Destek ve Ã‡Ã¶zÃ¼mler**\n\n**HÄ±zlÄ± Ã‡Ã¶zÃ¼mler:**\nâ€¢ Åžifre sÄ±fÄ±rlama: [Link]\nâ€¢ VPN kurulumu: [Rehber]\nâ€¢ Email ayarlarÄ±: [KÄ±lavuz]\n\n**Acil Durumlar:**\nâ€¢ Sistem Ã§Ã¶kmesi âžœ Hemen arayÄ±n\nâ€¢ GÃ¼venlik ihlali âžœ Acil bildirim\nâ€¢ Veri kaybÄ± âžœ Backup kurtarma\n\n**ðŸ“± Uzaktan Destek:**\nâ€¢ TeamViewer baÄŸlantÄ±sÄ±\nâ€¢ Ekran paylaÅŸÄ±mÄ±\nâ€¢ CanlÄ± rehberlik\n\nSorununuzu aÃ§Ä±klayÄ±n, Ã§Ã¶zÃ¼m bulalÄ±m!"
                ]
            },
            
            "company_info": {
                "patterns": [
                    "mefapex", "ÅŸirket", "company", "firma", "hakkÄ±nda", "about", 
                    "biliÅŸim teknolojileri", "nedir", "ne yapÄ±yor", "hizmet",
                    "teknoloji", "yazÄ±lÄ±m geliÅŸtirme"
                ],
                "responses": [
                    "ðŸ¢ **MEFAPEX BiliÅŸim Teknolojileri**\n\n**ðŸŽ¯ Misyonumuz:**\nModern teknoloji Ã§Ã¶zÃ¼mleri ile iÅŸletmelerin dijital dÃ¶nÃ¼ÅŸÃ¼mÃ¼nde Ã¶ncÃ¼ olmak\n\n**ðŸ’¼ Hizmet AlanlarÄ±mÄ±z:**\nâ€¢ **ðŸ–¥ï¸ YazÄ±lÄ±m GeliÅŸtirme**: Web, mobil, masaÃ¼stÃ¼ uygulamalar\nâ€¢ **â˜ï¸ Bulut Ã‡Ã¶zÃ¼mleri**: AWS, Azure, Google Cloud\nâ€¢ **ðŸ”’ Siber GÃ¼venlik**: Penetrasyon testleri, gÃ¼venlik danÄ±ÅŸmanlÄ±ÄŸÄ±\nâ€¢ **ðŸ“Š Veri Analizi**: Big Data, Business Intelligence\nâ€¢ **ðŸ¤– Yapay Zeka**: Machine Learning, chatbot geliÅŸtirme\nâ€¢ **ðŸŒ Sistem Entegrasyonu**: ERP, CRM sistemleri\n\n**ðŸŒŸ Neden MEFAPEX?**\nâ€¢ 10+ yÄ±l tecrÃ¼be\nâ€¢ ISO 27001 sertifikalÄ±\nâ€¢ 7/24 destek\nâ€¢ YenilikÃ§i Ã§Ã¶zÃ¼mler",
                    
                    "ðŸš€ **MEFAPEX: Teknolojide GÃ¼venilir Ã‡Ã¶zÃ¼m OrtaÄŸÄ±nÄ±z**\n\n**ðŸ“ˆ BaÅŸarÄ± Hikayeleri:**\nâ€¢ 500+ tamamlanan proje\nâ€¢ %98 mÃ¼ÅŸteri memnuniyeti\nâ€¢ 50+ teknoloji uzmanÄ±\n\n**ðŸ† Sertifikalar:**\nâ€¢ ISO 9001 Kalite YÃ¶netimi\nâ€¢ ISO 27001 Bilgi GÃ¼venliÄŸi\nâ€¢ Microsoft Partner\nâ€¢ AWS Partner Network\n\n**ðŸ“ Ä°letiÅŸim:**\nâ€¢ Merkez: Ä°stanbul\nâ€¢ Åžubeler: Ankara, Ä°zmir\nâ€¢ Web: www.mefapex.com\nâ€¢ Email: info@mefapex.com\n\nDaha detaylÄ± bilgi iÃ§in bizimle iletiÅŸime geÃ§in!"
                ]
            },
            
            "hr_processes": {
                "patterns": [
                    "izin", "leave", "tatil", "vacation", "baÅŸvuru", "application",
                    "personel", "ik", "human resources", "Ã¶zlÃ¼k", "bordro", "maaÅŸ"
                ],
                "responses": [
                    "ðŸ‘¥ **Ä°nsan KaynaklarÄ± SÃ¼reÃ§leri**\n\n**ðŸ“ Ä°zin BaÅŸvurularÄ±:**\nâ€¢ YÄ±llÄ±k izin: Online sistem Ã¼zerinden\nâ€¢ Mazeret izni: Ãœst amir onayÄ± ile\nâ€¢ HastalÄ±k izni: Rapor ibrazÄ±\n\n**ðŸ“‹ Gerekli Belgeler:**\nâ€¢ Ä°zin formu (doldurulmuÅŸ)\nâ€¢ Varsa tÄ±bbi rapor\nâ€¢ Ãœst amir onayÄ±\n\n**â±ï¸ BaÅŸvuru SÃ¼releri:**\nâ€¢ YÄ±llÄ±k izin: 15 gÃ¼n Ã¶nceden\nâ€¢ Acil izin: Amir onayÄ± ile\n\n**ðŸ“ž Ä°K Ä°letiÅŸim:**\nâ€¢ Ä°K DepartmanÄ±: [Telefon]\nâ€¢ Email: ik@mefapex.com\nâ€¢ Ofis: Ana bina 2. kat\n\nDetaylÄ± bilgi iÃ§in Ä°K departmanÄ±mÄ±zla iletiÅŸime geÃ§in.",
                    
                    "ðŸ“Š **Personel Ä°ÅŸlemleri ve Haklar**\n\n**ðŸ’° MaaÅŸ ve Ã–demeler:**\nâ€¢ Bordro: Her ayÄ±n 30'u\nâ€¢ Prim Ã¶demeleri: Performansa gÃ¶re\nâ€¢ Yan haklar: Yemek, ulaÅŸÄ±m, saÄŸlÄ±k\n\n**ðŸŽ“ EÄŸitim ve GeliÅŸim:**\nâ€¢ Teknik kurslar\nâ€¢ Sertifikasyon programlarÄ±\nâ€¢ Konferans katÄ±lÄ±mlarÄ±\n\n**ðŸ¥ Sosyal Haklar:**\nâ€¢ Ã–zel saÄŸlÄ±k sigortasÄ±\nâ€¢ Yemek kartÄ±\nâ€¢ UlaÅŸÄ±m desteÄŸi\nâ€¢ Spor Ã¼yeliÄŸi\n\nKiÅŸisel dosyanÄ±z iÃ§in Ä°K'ya baÅŸvurun."
                ]
            },
            
            "security_safety": {
                "patterns": [
                    "gÃ¼venlik", "security", "safety", "kural", "rule", "kaza", "accident",
                    "acil", "emergency", "koruyucu", "protective", "kask", "helmet"
                ],
                "responses": [
                    "ðŸ›¡ï¸ **Ä°ÅŸ GÃ¼venliÄŸi KurallarÄ±**\n\n**âš ï¸ Zorunlu Koruyucu Ekipmanlar:**\nâ€¢ ðŸ¦º Reflektif yelek\nâ€¢ ðŸ‘· GÃ¼venlik kaskÄ±\nâ€¢ ðŸ‘Ÿ GÃ¼venlik ayakkabÄ±sÄ±\nâ€¢ ðŸ§¤ Ä°ÅŸ eldivenleri\nâ€¢ ðŸ¥½ Koruyucu gÃ¶zlÃ¼k\n\n**ðŸš¨ Acil Durum ProsedÃ¼rleri:**\nâ€¢ YangÄ±n: Alarm Ã§al, binayÄ± boÅŸalt\nâ€¢ Kaza: Ä°lk yardÄ±m, 112'yi ara\nâ€¢ GÃ¼venlik ihlali: GÃ¼venliÄŸi bilgilendir\n\n**ðŸ“ž Acil Numaralar:**\nâ€¢ Ä°tfaiye: 110\nâ€¢ Ambulans: 112\nâ€¢ Polis: 155\nâ€¢ Ä°Ã§ hat gÃ¼venlik: [Dahili]\n\n**ðŸ“‹ GÃ¼venlik EÄŸitimleri:**\nâ€¢ AylÄ±k gÃ¼venlik toplantÄ±larÄ±\nâ€¢ Ä°lk yardÄ±m sertifikasÄ±\nâ€¢ YangÄ±n tatbikatlarÄ±",
                    
                    "ðŸ”’ **GÃ¼venlik Protokolleri ve Ã–nlemler**\n\n**ðŸ­ Fabrika GÃ¼venliÄŸi:**\nâ€¢ Makine Ã§alÄ±ÅŸÄ±rken yaklaÅŸma\nâ€¢ GÃ¼venlik bariyerlerine uy\nâ€¢ UyarÄ± levhalarÄ±nÄ± oku\nâ€¢ Temizlik maddelerini dikkatli kullan\n\n**ðŸ’» Bilgi GÃ¼venliÄŸi:**\nâ€¢ GÃ¼Ã§lÃ¼ ÅŸifreler kullan\nâ€¢ 2FA (iki faktÃ¶rlÃ¼ doÄŸrulama) aktif et\nâ€¢ ÅžÃ¼pheli emailleri aÃ§ma\nâ€¢ USB cihazlarÄ± kontrol et\n\n**ðŸ†” EriÅŸim GÃ¼venliÄŸi:**\nâ€¢ KartÄ±nÄ± baÅŸkasÄ±na verme\nâ€¢ KapÄ±larÄ± arkandan kitle\nâ€¢ ZiyaretÃ§ileri kaydet\n\nGÃ¼venlik sizin sorumluluÄŸunuz!"
                ]
            },
            
            "thanks_goodbye": {
                "patterns": [
                    "teÅŸekkÃ¼r", "thanks", "saÄŸol", "thank you", "bye", "gÃ¶rÃ¼ÅŸÃ¼rÃ¼z",
                    "hoÅŸÃ§a kal", "goodbye", "elveda", "iyi gÃ¼nler"
                ],
                "responses": [
                    "ðŸ™ **Rica ederim!**\n\nSize yardÄ±mcÄ± olabildiysem ne mutlu bana! \n\n**ðŸ“ž Ä°htiyaÃ§ halinde:**\nâ€¢ Teknik destek: destek@mefapex.com\nâ€¢ Genel bilgi: info@mefapex.com\nâ€¢ Acil durumlar: [Acil telefon]\n\nBaÅŸka sorularÄ±nÄ±z olduÄŸunda her zaman buradayÄ±m.\n\nâœ¨ **MEFAPEX ailesinin bir parÃ§asÄ± olduÄŸunuz iÃ§in teÅŸekkÃ¼rler!**\n\nÄ°yi Ã§alÄ±ÅŸmalar dilerim! ðŸŒŸ",
                    
                    "ðŸ˜Š **Her zaman yardÄ±mcÄ± olmaktan mutluluk duyarÄ±m!**\n\nUmarÄ±m sorularÄ±nÄ±za faydalÄ± yanÄ±tlar verebildim.\n\n**ðŸ”— BaÄŸlantÄ±da KalÄ±n:**\nâ€¢ Web: www.mefapex.com\nâ€¢ LinkedIn: MEFAPEX BiliÅŸim\nâ€¢ Email: info@mefapex.com\n\n**ðŸ’¡ Ä°pucu:** Favorilerinize ekleyerek bana daha kolay ulaÅŸabilirsiniz!\n\nGÃ¶rÃ¼ÅŸÃ¼rÃ¼z! ðŸ‘‹"
                ]
            },
            
            # Yeni kategoriler
            "project_management": {
                "patterns": [
                    "proje", "project", "gÃ¶rev", "task", "deadline", "son tarih",
                    "teslim", "delivery", "milestone", "plan", "planlama"
                ],
                "responses": [
                    "ðŸ“‹ **Proje YÃ¶netimi ve SÃ¼reÃ§ler**\n\n**ðŸŽ¯ Proje AÅŸamalarÄ±:**\nâ€¢ **Analiz**: Ä°htiyaÃ§ belirleme ve dokÃ¼mantasyon\nâ€¢ **TasarÄ±m**: Sistem mimarisi ve UI/UX\nâ€¢ **GeliÅŸtirme**: Kodlama ve test sÃ¼reÃ§leri\nâ€¢ **Test**: QA ve kullanÄ±cÄ± testleri\nâ€¢ **Teslim**: CanlÄ±ya alma ve eÄŸitim\n\n**ðŸ“Š Proje Takibi:**\nâ€¢ HaftalÄ±k durum raporlarÄ±\nâ€¢ Sprint toplantÄ±larÄ±\nâ€¢ Milestone deÄŸerlendirmeleri\n\n**ðŸ”§ KullanÄ±lan AraÃ§lar:**\nâ€¢ Jira - GÃ¶rev yÃ¶netimi\nâ€¢ Confluence - DokÃ¼mantasyon\nâ€¢ Git - Versiyon kontrolÃ¼\nâ€¢ Slack - Ä°letiÅŸim\n\nProje durumunuz iÃ§in proje yÃ¶neticisiyle iletiÅŸime geÃ§in."
                ]
            },
            
            "training_education": {
                "patterns": [
                    "eÄŸitim", "training", "kurs", "course", "Ã¶ÄŸren", "learn",
                    "sertifika", "certificate", "rehber", "guide", "nasÄ±l"
                ],
                "responses": [
                    "ðŸŽ“ **EÄŸitim ve GeliÅŸim ProgramlarÄ±**\n\n**ðŸ’» Teknik EÄŸitimler:**\nâ€¢ **Programlama**: Python, JavaScript, C#\nâ€¢ **VeritabanÄ±**: SQL, MongoDB, PostgreSQL\nâ€¢ **Bulut**: AWS, Azure, Google Cloud\nâ€¢ **DevOps**: Docker, Kubernetes, CI/CD\n\n**ðŸ“š KiÅŸisel GeliÅŸim:**\nâ€¢ Proje yÃ¶netimi\nâ€¢ Ä°letiÅŸim becerileri\nâ€¢ Liderlik eÄŸitimleri\nâ€¢ Agile metodolojiler\n\n**ðŸ† Sertifikasyon DesteÄŸi:**\nâ€¢ SÄ±nav Ã¼cretleri karÅŸÄ±lanÄ±r\nâ€¢ HazÄ±rlÄ±k kurslarÄ±\nâ€¢ Mentorluk programÄ±\n\n**ðŸ“ž EÄŸitim KoordinatÃ¶rÃ¼:**\nâ€¢ Email: egitim@mefapex.com\nâ€¢ Dahili: [Telefon]\n\nKariyerinizi geliÅŸtirmek iÃ§in bizimle iletiÅŸime geÃ§in!"
                ]
            }
        }
    
    def _create_synonym_map(self) -> Dict[str, List[str]]:
        """TÃ¼rkÃ§e eÅŸ anlamlÄ± kelimeler"""
        return {
            'Ã§alÄ±ÅŸma': ['iÅŸ', 'mesai', 'gÃ¶rev', 'vazife', 'work', 'job'],
            'saat': ['zaman', 'vakit', 'time', 'hours', 'hour'],
            'aÃ§Ä±k': ['open', 'baÅŸlama', 'start', 'begin'],
            'kapalÄ±': ['closed', 'bitiÅŸ', 'end', 'finish', 'stop'],
            'destek': ['yardÄ±m', 'help', 'support', 'assistance', 'aid'],
            'problem': ['sorun', 'hata', 'error', 'issue', 'bug'],
            'mefapex': ['ÅŸirket', 'company', 'firma', 'organization', 'kurum'],
            'gÃ¼venlik': ['security', 'safety', 'emniyet', 'koruma'],
            'izin': ['leave', 'vacation', 'tatil', 'permit', 'permission'],
            'proje': ['project', 'gÃ¶rev', 'task', 'iÅŸ', 'work'],
            'eÄŸitim': ['training', 'education', 'kurs', 'course', 'Ã¶ÄŸretim'],
            'sistem': ['system', 'software', 'yazÄ±lÄ±m', 'program'],
            'hata': ['error', 'bug', 'problem', 'sorun', 'issue'],
            'yardÄ±m': ['help', 'support', 'destek', 'assistance']
        }
    
    def _normalize_turkish(self, text: str) -> str:
        """TÃ¼rkÃ§e karakterleri normalize et ve morfological analysis uygula"""
        if not text:
            return text
        
        # Ã–nce tÃ¼rkÃ§e karakter dÃ¶nÃ¼ÅŸÃ¼mleri
        turkish_map = {
            'Ã§': 'c', 'ÄŸ': 'g', 'Ä±': 'i', 'Ã¶': 'o', 'ÅŸ': 's', 'Ã¼': 'u',
            'Ã‡': 'c', 'Äž': 'g', 'I': 'i', 'Ä°': 'i', 'Ã–': 'o', 'Åž': 's', 'Ãœ': 'u'
        }
        
        normalized = ""
        for char in text.lower():
            normalized += turkish_map.get(char, char)
        
        return normalized
    
    def _preprocess_text(self, text: str) -> str:
        """Metni Ã¶niÅŸleme: temizleme + normalizasyon + lemmatization"""
        if not text:
            return text
        
        # Temizleme
        text = text.strip().lower()
        text = re.sub(r'[^\w\s]', ' ', text)  # Noktalama iÅŸaretlerini kaldÄ±r
        text = re.sub(r'\s+', ' ', text)  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        
        # Lemmatization
        lemmatized_text = self.morph_analyzer.lemmatize_text(text)
        
        # TÃ¼rkÃ§e karakter normalizasyonu (son adÄ±m olarak)
        normalized_text = self._normalize_turkish(lemmatized_text)
        
        return normalized_text
    
    def _expand_with_synonyms(self, text: str) -> List[str]:
        """Metni eÅŸ anlamlÄ± kelimelerle ve morfological varyantlarla geniÅŸlet"""
        words = text.split()
        expanded_texts = {text}  # Set kullanarak dublicate'leri Ã¶nle
        
        # Her kelime iÃ§in eÅŸ anlamlÄ±larÄ± ve varyantlarÄ±nÄ± kontrol et
        for word in words:
            # Kelimeyi lemmatize et
            lemma = self.morph_analyzer.lemmatize_word(word)
            
            # Normalized versiyonlarÄ± da kontrol et
            normalized_word = self._normalize_turkish(word)
            normalized_lemma = self._normalize_turkish(lemma)
            
            # EÅŸ anlamlÄ±larÄ± bul
            synonyms_found = set()
            
            # Orijinal kelime iÃ§in eÅŸ anlamlÄ±larÄ±
            for key in [word, lemma, normalized_word, normalized_lemma]:
                if key in self.synonym_map:
                    synonyms_found.update(self.synonym_map[key])
            
            # Morfological varyantlarÄ± da ekle
            variants = self.morph_analyzer.get_word_variants(word)
            for variant in variants:
                normalized_variant = self._normalize_turkish(variant)
                for key in [variant, normalized_variant]:
                    if key in self.synonym_map:
                        synonyms_found.update(self.synonym_map[key])
            
            # Bulunan eÅŸ anlamlÄ±larÄ± ile yeni metinler oluÅŸtur
            for synonym in synonyms_found:
                if synonym and synonym != word:
                    # Orijinal kelimeyi eÅŸ anlamlÄ±sÄ±yla deÄŸiÅŸtir
                    new_text = text.replace(word, synonym)
                    expanded_texts.add(new_text)
                    
                    # Lemmatized versiyonlarÄ±nÄ± da dene
                    lemma_text = text.replace(word, self.morph_analyzer.lemmatize_word(synonym))
                    expanded_texts.add(lemma_text)
        
        return list(expanded_texts)
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Ä°ki metin arasÄ±nda morfological analysis destekli benzerlik hesapla"""
        # Cache kontrolÃ¼
        cache_key = f"{text1}|{text2}"
        if cache_key in self.pattern_cache:
            return self.pattern_cache[cache_key]
        
        # Metinleri Ã¶niÅŸle
        processed1 = self._preprocess_text(text1)
        processed2 = self._preprocess_text(text2)
        
        # Temel benzerlik
        basic_similarity = SequenceMatcher(None, processed1, processed2).ratio()
        
        # Kelime bazlÄ± benzerlik (lemmatized)
        words1 = set(processed1.split())
        words2 = set(processed2.split())
        
        if not words1 or not words2:
            similarity = basic_similarity
        else:
            # Exact match
            word_similarity = len(words1 & words2) / len(words1 | words2)
            
            # Semantic similarity (synonym-based)
            semantic_matches = 0
            total_comparisons = 0
            
            for w1 in words1:
                for w2 in words2:
                    total_comparisons += 1
                    if w1 == w2:
                        semantic_matches += 1
                    else:
                        # Check if words are synonyms
                        w1_synonyms = set()
                        w2_synonyms = set()
                        
                        # Collect synonyms for w1
                        for key, synonyms in self.synonym_map.items():
                            if w1 in synonyms or w1 == key:
                                w1_synonyms.update(synonyms)
                                w1_synonyms.add(key)
                        
                        # Collect synonyms for w2
                        for key, synonyms in self.synonym_map.items():
                            if w2 in synonyms or w2 == key:
                                w2_synonyms.update(synonyms)
                                w2_synonyms.add(key)
                        
                        # Check if there's overlap
                        if w1_synonyms & w2_synonyms:
                            semantic_matches += 0.8  # Synonym match weight
            
            if total_comparisons > 0:
                semantic_similarity = semantic_matches / total_comparisons
            else:
                semantic_similarity = 0
            
            # AÄŸÄ±rlÄ±klÄ± ortalama
            similarity = (basic_similarity * 0.4) + (word_similarity * 0.4) + (semantic_similarity * 0.2)
        
        # Cache'e kaydet
        self.pattern_cache[cache_key] = similarity
        return similarity
    
    def find_best_match(self, user_input: str, threshold: float = 0.3) -> Optional[Dict]:
        """KullanÄ±cÄ± girdisi iÃ§in en iyi eÅŸleÅŸmeyi bul (morfological analysis ile)"""
        if not user_input or not user_input.strip():
            return None
        
        # KullanÄ±cÄ± girdisini Ã¶niÅŸle
        processed_input = self._preprocess_text(user_input)
        
        best_match = None
        best_score = 0.0
        
        # EÅŸ anlamlÄ± kelimelerle ve morfological varyantlarla geniÅŸletilmiÅŸ versiyonlarÄ± da kontrol et
        expanded_inputs = self._expand_with_synonyms(processed_input)
        
        # Orijinal girdiyi de ekle
        all_inputs = [user_input.strip().lower(), processed_input] + expanded_inputs
        all_inputs = list(set(all_inputs))  # Duplicate'leri kaldÄ±r
        
        for category, data in self.responses.items():
            patterns = data["patterns"]
            responses = data["responses"]
            
            for pattern in patterns:
                # Pattern'i de Ã¶niÅŸle
                processed_pattern = self._preprocess_text(pattern)
                
                # TÃ¼m girdi versiyonlarÄ±nÄ± kontrol et
                for input_variant in all_inputs:
                    # Direkt iÃ§erme kontrolÃ¼ (yÃ¼ksek puan)
                    if processed_pattern in input_variant or input_variant in processed_pattern:
                        score = 0.95
                    elif pattern.lower() in user_input.lower() or user_input.lower() in pattern.lower():
                        score = 0.9  # Orijinal metin eÅŸleÅŸmesi
                    else:
                        # Morfological similarity
                        score = self._calculate_similarity(input_variant, processed_pattern)
                    
                    if score > best_score and score >= threshold:
                        best_score = score
                        best_match = {
                            "category": category,
                            "pattern": pattern,
                            "responses": responses,
                            "score": score,
                            "matched_input": input_variant,
                            "processed_pattern": processed_pattern
                        }
        
        # EÄŸer hala eÅŸleÅŸme yoksa, daha esnek arama yap
        if not best_match or best_score < 0.5:
            # Lemmatized kelime bazlÄ± arama
            input_lemmas = set(self._preprocess_text(user_input).split())
            
            for category, data in self.responses.items():
                patterns = data["patterns"]
                responses = data["responses"]
                
                for pattern in patterns:
                    pattern_lemmas = set(self._preprocess_text(pattern).split())
                    
                    if input_lemmas and pattern_lemmas:
                        # Lemma intersection score
                        intersection = len(input_lemmas & pattern_lemmas)
                        union = len(input_lemmas | pattern_lemmas)
                        
                        if union > 0:
                            lemma_score = intersection / union
                            
                            # Bonus for multiple word matches
                            if intersection > 1:
                                lemma_score *= 1.2
                            
                            if lemma_score > best_score and lemma_score >= (threshold * 0.8):
                                best_score = lemma_score
                                best_match = {
                                    "category": category,
                                    "pattern": pattern,
                                    "responses": responses,
                                    "score": lemma_score,
                                    "matched_input": user_input,
                                    "processed_pattern": pattern,
                                    "match_type": "lemma_based"
                                }
        
        return best_match
    
    def get_response(self, user_input: str) -> str:
        """KullanÄ±cÄ± girdisi iÃ§in yanÄ±t Ã¼ret"""
        # Ã–nce static responses'dan ara
        match = self.find_best_match(user_input)
        
        if match:
            responses = match["responses"]
            import random
            response = random.choice(responses)
            logger.info(f"ðŸŽ¯ Static response matched: {match['category']} (score: {match['score']:.2f})")
            return response
        
        # Fallback yanÄ±tlarÄ±
        return self._get_fallback_response(user_input)
    
    def _get_fallback_response(self, user_input: str) -> str:
        """EÅŸleÅŸme bulunamadÄ±ÄŸÄ±nda fallback yanÄ±tlarÄ±"""
        fallback_responses = [
            f"ðŸ¤” **\"{user_input}\" konusunda size yardÄ±mcÄ± olmaya Ã§alÄ±ÅŸÄ±yorum...**\n\n"
            "Bu konuda elimde yeterli bilgi bulunmuyor, ancak size ÅŸu ÅŸekillerde yardÄ±mcÄ± olabilirim:\n\n"
            "â€¢ ðŸ“ž **Teknik Destek**: destek@mefapex.com\n"
            "â€¢ ðŸ’¬ **Genel Sorular**: info@mefapex.com\n"
            "â€¢ ðŸ†˜ **Acil Durumlar**: [Acil telefon]\n\n"
            "LÃ¼tfen sorunuzu daha detaylandÄ±rÄ±r mÄ±sÄ±nÄ±z? BÃ¶ylece size daha iyi yardÄ±mcÄ± olabilirim.",
            
            f"ðŸ’¡ **\"{user_input}\" hakkÄ±nda bilgi arÄ±yorsunuz...**\n\n"
            "Bu konu iÃ§in size en doÄŸru bilgiyi ÅŸu kanallardan alabilirsiniz:\n\n"
            "â€¢ ðŸ¢ **Ä°lgili Departman**: Konuyla ilgili uzman ekibimiz\n"
            "â€¢ ðŸ“‹ **DokÃ¼mantasyon**: Åžirket iÃ§i rehberler\n"
            "â€¢ ðŸ‘¥ **MeslektaÅŸlarÄ±nÄ±z**: Deneyimli ekip Ã¼yeleri\n\n"
            "Ben de Ã¶ÄŸrenmeye devam ediyorum. Sorunuzu biraz daha aÃ§abilir misiniz?",
            
            f"ðŸ” **\"{user_input}\" konusunu araÅŸtÄ±rÄ±yorum...**\n\n"
            "Åžu anda bu konuda size kapsamlÄ± bir yanÄ±t veremiyorum, ama:\n\n"
            "â€¢ ðŸ“ž **HÄ±zlÄ± Ã‡Ã¶zÃ¼m**: Ä°lgili departmanÄ± arayÄ±n\n"
            "â€¢ ðŸ’» **Online Kaynaklar**: Åžirket portalÄ±nÄ± kontrol edin\n"
            "â€¢ ðŸ“ **Ticket AÃ§Ä±n**: Sistemden destek talebi oluÅŸturun\n\n"
            "Bu arada bana daha spesifik sorular sorabilirsiniz. Ã–rneÄŸin teknik, Ä°K, gÃ¼venlik konularÄ±nda size yardÄ±mcÄ± olabilirim!"
        ]
        
        import random
        return random.choice(fallback_responses)
    
    def get_category_suggestions(self, user_input: str) -> List[str]:
        """KullanÄ±cÄ± girdisine gÃ¶re kategori Ã¶nerileri"""
        suggestions = []
        user_input_lower = user_input.lower()
        
        # Anahtar kelimeler bazÄ±nda Ã¶neriler
        if any(word in user_input_lower for word in ['saat', 'Ã§alÄ±ÅŸma', 'mesai', 'aÃ§Ä±k', 'kapalÄ±']):
            suggestions.append("Ã‡alÄ±ÅŸma saatleri hakkÄ±nda bilgi almak iÃ§in: 'Ã§alÄ±ÅŸma saatleri' yazÄ±n")
        
        if any(word in user_input_lower for word in ['destek', 'help', 'problem', 'hata']):
            suggestions.append("Teknik destek iÃ§in: 'teknik destek' yazÄ±n")
        
        if any(word in user_input_lower for word in ['izin', 'tatil', 'ik', 'personel']):
            suggestions.append("Ä°nsan kaynaklarÄ± iÃ§in: 'izin baÅŸvurusu' yazÄ±n")
        
        if any(word in user_input_lower for word in ['gÃ¼venlik', 'kural', 'kaza']):
            suggestions.append("GÃ¼venlik bilgileri iÃ§in: 'gÃ¼venlik kurallarÄ±' yazÄ±n")
        
        if any(word in user_input_lower for word in ['ÅŸirket', 'mefapex', 'hakkÄ±nda']):
            suggestions.append("Åžirket bilgileri iÃ§in: 'MEFAPEX hakkÄ±nda' yazÄ±n")
        
        return suggestions[:3]  # En fazla 3 Ã¶neri
    
    def get_stats(self) -> Dict:
        """Content manager istatistikleri"""
        total_patterns = sum(len(data["patterns"]) for data in self.responses.values())
        total_responses = sum(len(data["responses"]) for data in self.responses.values())
        
        # Morfological analyzer stats
        morph_stats = {
            "spacy_available": self.morph_analyzer.nlp is not None,
            "fallback_lemmas": len(self.morph_analyzer.fallback_lemmas),
            "morphological_analysis": "Enabled" if SPACY_AVAILABLE else "Fallback mode"
        }
        
        return {
            "total_categories": len(self.responses),
            "total_patterns": total_patterns,
            "total_responses": total_responses,
            "cache_size": len(self.pattern_cache),
            "synonym_words": len(self.synonym_map),
            "language": "Turkish (Enhanced with Morphological Analysis)",
            "morphological_analysis": morph_stats
        }

# Global instance
improved_turkish_content = ImprovedTurkishContentManager()
